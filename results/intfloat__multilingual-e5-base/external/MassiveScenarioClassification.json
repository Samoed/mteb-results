{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 65.08742434431743,
                "f1": 63.044060042311756,
                "main_score": 65.08742434431743
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 58.52387357094821,
                "f1": 56.82398588814534,
                "main_score": 58.52387357094821
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 62.239408204438476,
                "f1": 61.92570286170469,
                "main_score": 62.239408204438476
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 63.74915938130463,
                "f1": 62.130740689396276,
                "main_score": 63.74915938130463
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 65.00336247478144,
                "f1": 63.71080635228055,
                "main_score": 65.00336247478144
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 52.837928715534645,
                "f1": 50.390741680320836,
                "main_score": 52.837928715534645
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 72.42098184263618,
                "f1": 71.41355113538995,
                "main_score": 72.42098184263618
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 71.95359784801613,
                "f1": 71.42699340156742,
                "main_score": 71.95359784801613
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 70.18157363819772,
                "f1": 69.74836113037671,
                "main_score": 70.18157363819772
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 77.08137188971082,
                "f1": 76.78000685068261,
                "main_score": 77.08137188971082
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 71.5030262273033,
                "f1": 71.71620130425673,
                "main_score": 71.5030262273033
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 70.24546065904505,
                "f1": 69.07638311730359,
                "main_score": 70.24546065904505
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 69.12911903160726,
                "f1": 68.32651736539815,
                "main_score": 69.12911903160726
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 71.89307330195025,
                "f1": 71.33986549860187,
                "main_score": 71.89307330195025
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 67.44451916610626,
                "f1": 66.90192664503866,
                "main_score": 67.44451916610626
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 69.16274377942166,
                "f1": 68.01090953775066,
                "main_score": 69.16274377942166
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 70.75319435104237,
                "f1": 70.18035309201403,
                "main_score": 70.75319435104237
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 63.14391392064559,
                "f1": 61.48286540778145,
                "main_score": 63.14391392064559
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 70.70275722932078,
                "f1": 70.26164779846495,
                "main_score": 70.70275722932078
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 60.93813046402153,
                "f1": 58.8852862116525,
                "main_score": 60.93813046402153
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 72.320107599193,
                "f1": 72.19836409602924,
                "main_score": 72.320107599193
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 74.65366509751176,
                "f1": 74.55188288799579,
                "main_score": 74.65366509751176
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 59.694014794889036,
                "f1": 58.11353311721067,
                "main_score": 59.694014794889036
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 54.37457969065231,
                "f1": 52.81306134311697,
                "main_score": 54.37457969065231
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 48.3086751849361,
                "f1": 45.396449765419376,
                "main_score": 48.3086751849361
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 62.151983860121064,
                "f1": 60.31762544281696,
                "main_score": 62.151983860121064
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 72.44788164088769,
                "f1": 71.68150151736367,
                "main_score": 72.44788164088769
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 62.81439139206455,
                "f1": 62.06735559105593,
                "main_score": 62.81439139206455
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 68.04303967720242,
                "f1": 66.68298851670133,
                "main_score": 68.04303967720242
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 61.43913920645595,
                "f1": 60.25605977560783,
                "main_score": 61.43913920645595
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 66.90316072629456,
                "f1": 65.1325924692381,
                "main_score": 66.90316072629456
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 61.63752521856086,
                "f1": 59.14284778039585,
                "main_score": 61.63752521856086
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 71.63080026899797,
                "f1": 70.89771864626877,
                "main_score": 71.63080026899797
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 72.10827168796234,
                "f1": 71.71954219691159,
                "main_score": 72.10827168796234
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 70.59515803631471,
                "f1": 70.05040128099003,
                "main_score": 70.59515803631471
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 70.83389374579691,
                "f1": 70.84877936562735,
                "main_score": 70.83389374579691
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 69.18628110289173,
                "f1": 68.97232927921841,
                "main_score": 69.18628110289173
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 72.99260255548083,
                "f1": 72.85139492157732,
                "main_score": 72.99260255548083
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 65.26227303295225,
                "f1": 65.08833655469431,
                "main_score": 65.26227303295225
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 66.48621385339611,
                "f1": 64.43483199071298,
                "main_score": 66.48621385339611
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 73.14391392064559,
                "f1": 72.2580822579741,
                "main_score": 73.14391392064559
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 59.88567585743107,
                "f1": 58.3073765932569,
                "main_score": 59.88567585743107
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 62.38399462004034,
                "f1": 60.82139544252606,
                "main_score": 62.38399462004034
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 62.58574310692671,
                "f1": 60.71443370385374,
                "main_score": 62.58574310692671
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 71.61398789509079,
                "f1": 70.99761812049401,
                "main_score": 71.61398789509079
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 62.73705447209146,
                "f1": 61.680849331794796,
                "main_score": 62.73705447209146
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 71.66778749159381,
                "f1": 71.17320646080115,
                "main_score": 71.66778749159381
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 64.640215198386,
                "f1": 63.301805157015444,
                "main_score": 64.640215198386
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 70.00672494956288,
                "f1": 70.26005548582106,
                "main_score": 70.00672494956288
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 75.42030934767989,
                "f1": 75.2074842882598,
                "main_score": 75.42030934767989
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 70.69266980497646,
                "f1": 70.94103167391192,
                "main_score": 70.69266980497646
            }
        ]
    }
}