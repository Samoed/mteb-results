{
    "dataset_revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7",
    "task_name": "MassiveIntentClassification",
    "evaluation_time": NaN,
    "mteb_version": "unknown",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 25.79690652320108,
                "f1": 24.093438782440067,
                "main_score": 25.79690652320108
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 3.338937457969066,
                "f1": 2.404152046553366,
                "main_score": 3.338937457969066
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 6.489576328177541,
                "f1": 4.62270646032821,
                "main_score": 6.489576328177541
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 24.767989240080695,
                "f1": 23.495689794075474,
                "main_score": 24.767989240080695
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 4.29724277067922,
                "f1": 2.2466735164037934,
                "main_score": 4.29724277067922
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 26.388702084734366,
                "f1": 23.86003112409349,
                "main_score": 26.388702084734366
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 31.4694014794889,
                "f1": 29.017559554815392,
                "main_score": 31.4694014794889
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 28.09011432414256,
                "f1": 24.796051996220104,
                "main_score": 28.09011432414256
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 19.240080699394753,
                "f1": 16.13607169381968,
                "main_score": 19.240080699394753
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 53.406186953597846,
                "f1": 49.55550114595557,
                "main_score": 53.406186953597846
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 30.615332885003365,
                "f1": 29.13481030937436,
                "main_score": 30.615332885003365
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 7.205783456624077,
                "f1": 4.601802513446058,
                "main_score": 7.205783456624077
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 27.205783456624072,
                "f1": 24.177535740725418,
                "main_score": 27.205783456624072
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 32.63618022864828,
                "f1": 31.190168140021303,
                "main_score": 32.63618022864828
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 2.6630800268997983,
                "f1": 1.913464455449111,
                "main_score": 2.6630800268997983
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 4.593140551445864,
                "f1": 2.6428594688121865,
                "main_score": 4.593140551445864
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 25.648957632817755,
                "f1": 22.88249345748577,
                "main_score": 25.648957632817755
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 4.862138533960995,
                "f1": 2.739262235100375,
                "main_score": 4.862138533960995
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 29.808338937457968,
                "f1": 29.055301842025006,
                "main_score": 29.808338937457968
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 23.5305985205111,
                "f1": 21.232935753763023,
                "main_score": 23.5305985205111
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 34.47209145931405,
                "f1": 32.987844813265305,
                "main_score": 34.47209145931405
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 39.40147948890383,
                "f1": 37.59135086216479,
                "main_score": 39.40147948890383
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 28.749159381304644,
                "f1": 26.132814845473103,
                "main_score": 28.749159381304644
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 4.344317417619368,
                "f1": 2.9377190150068957,
                "main_score": 4.344317417619368
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 6.102891728312036,
                "f1": 3.962539148306579,
                "main_score": 6.102891728312036
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 4.462004034969738,
                "f1": 2.618811361288446,
                "main_score": 4.462004034969738
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 14.156018829858777,
                "f1": 12.032224251194693,
                "main_score": 14.156018829858777
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 29.855413584398114,
                "f1": 26.4935948293649,
                "main_score": 29.855413584398114
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 3.6919973100201746,
                "f1": 1.9455177645088522,
                "main_score": 3.6919973100201746
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 7.864828513786145,
                "f1": 5.890463192551815,
                "main_score": 7.864828513786145
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 28.05312710154674,
                "f1": 25.167919685520967,
                "main_score": 28.05312710154674
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 6.9804976462676525,
                "f1": 3.5784028024922963,
                "main_score": 6.9804976462676525
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 28.648285137861468,
                "f1": 26.310748922753458,
                "main_score": 28.648285137861468
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 32.92199058507062,
                "f1": 29.644293217296745,
                "main_score": 32.92199058507062
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 27.622730329522525,
                "f1": 24.469668239014812,
                "main_score": 27.622730329522525
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 33.52723604572966,
                "f1": 30.92548605264693,
                "main_score": 33.52723604572966
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 31.32145258910558,
                "f1": 29.33012083327001,
                "main_score": 31.32145258910558
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 11.271015467383993,
                "f1": 10.062644252034659,
                "main_score": 11.271015467383993
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 27.942165433759254,
                "f1": 25.33080090111651,
                "main_score": 27.942165433759254
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 32.895090786819104,
                "f1": 28.439539068323533,
                "main_score": 32.895090786819104
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 26.852723604572965,
                "f1": 25.395656180022463,
                "main_score": 26.852723604572965
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 29.404841963685275,
                "f1": 26.21621146728871,
                "main_score": 29.404841963685275
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 3.3254875588433084,
                "f1": 1.603092746324846,
                "main_score": 3.3254875588433084
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 3.463349024882313,
                "f1": 2.0928407358339736,
                "main_score": 3.463349024882313
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 12.982515131136513,
                "f1": 11.01712677163494,
                "main_score": 12.982515131136513
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 30.729657027572294,
                "f1": 26.83317869501593,
                "main_score": 30.729657027572294
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 23.570948217888372,
                "f1": 22.40127425434381,
                "main_score": 23.570948217888372
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 4.983187626092804,
                "f1": 3.718347039099332,
                "main_score": 4.983187626092804
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 21.893073301950235,
                "f1": 20.675025999929755,
                "main_score": 21.893073301950235
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 70.82044384667114,
                "f1": 68.45757969594844,
                "main_score": 70.82044384667114
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 65.52790854068594,
                "f1": 64.67811461805108,
                "main_score": 65.52790854068594
            }
        ]
    }
}