{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 31.55346334902488,
                "f1": 29.191252426466214,
                "main_score": 31.55346334902488
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 7.488231338264964,
                "f1": 5.0160562127811685,
                "main_score": 7.488231338264964
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 14.996637525218562,
                "f1": 11.249193510499182,
                "main_score": 14.996637525218562
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 29.132481506388704,
                "f1": 26.38076542865094,
                "main_score": 29.132481506388704
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 9.243443174176194,
                "f1": 6.038537369329211,
                "main_score": 9.243443174176194
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 29.720914593140552,
                "f1": 26.928872478782672,
                "main_score": 29.720914593140552
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 39.791526563550775,
                "f1": 35.368330225384334,
                "main_score": 39.791526563550775
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 34.67720242098184,
                "f1": 31.39329003485599,
                "main_score": 34.67720242098184
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 28.833221250840612,
                "f1": 25.447170771092875,
                "main_score": 28.833221250840612
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 61.77538668459987,
                "f1": 58.638713385209904,
                "main_score": 61.77538668459987
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 35.96839273705447,
                "f1": 32.76368046131194,
                "main_score": 35.96839273705447
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 11.123066577000671,
                "f1": 7.872676893973924,
                "main_score": 11.123066577000671
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 28.614660390047074,
                "f1": 25.645947438401855,
                "main_score": 28.614660390047074
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 40.65568258238063,
                "f1": 37.66349602390266,
                "main_score": 40.65568258238063
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 9.008069939475453,
                "f1": 5.1554089293899965,
                "main_score": 9.008069939475453
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 9.915938130464022,
                "f1": 6.413016136448915,
                "main_score": 9.915938130464022
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 32.07464694014795,
                "f1": 29.435621484770174,
                "main_score": 32.07464694014795
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 8.43981170141224,
                "f1": 6.157676936089851,
                "main_score": 8.43981170141224
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 34.89912575655683,
                "f1": 31.614865711813938,
                "main_score": 34.89912575655683
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 30.954942837928712,
                "f1": 26.844232924337497,
                "main_score": 30.954942837928712
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 41.059179556153325,
                "f1": 36.92471115077745,
                "main_score": 41.059179556153325
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 48.73234700739745,
                "f1": 46.89395011365762,
                "main_score": 48.73234700739745
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 35.08742434431742,
                "f1": 32.314520222383585,
                "main_score": 35.08742434431742
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 9.287155346334904,
                "f1": 7.13459214077311,
                "main_score": 9.287155346334904
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 11.193678547410894,
                "f1": 7.329043540295746,
                "main_score": 11.193678547410894
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 10.097511768661734,
                "f1": 7.212356186519867,
                "main_score": 10.097511768661734
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 19.196368527236046,
                "f1": 16.798046606500282,
                "main_score": 19.196368527236046
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 32.49495628782785,
                "f1": 28.359188240241444,
                "main_score": 32.49495628782785
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 6.36516476126429,
                "f1": 3.7192665599079913,
                "main_score": 6.36516476126429
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 13.076664425016812,
                "f1": 9.572770203976713,
                "main_score": 13.076664425016812
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 39.17955615332885,
                "f1": 33.8253960820197,
                "main_score": 39.17955615332885
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 12.252858103564224,
                "f1": 9.096519579346872,
                "main_score": 12.252858103564224
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 35.24209818426362,
                "f1": 32.24756964062884,
                "main_score": 35.24209818426362
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 38.17081371889711,
                "f1": 34.8539465599922,
                "main_score": 38.17081371889711
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 31.60390047074647,
                "f1": 28.24199310436465,
                "main_score": 31.60390047074647
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 40.01008742434432,
                "f1": 37.21826826542489,
                "main_score": 40.01008742434432
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 39.25353059852051,
                "f1": 35.457426597271784,
                "main_score": 39.25353059852051
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 16.70813718897108,
                "f1": 14.338767956114001,
                "main_score": 16.70813718897108
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 33.94418291862811,
                "f1": 30.577444242695694,
                "main_score": 33.94418291862811
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 40.396772024209824,
                "f1": 36.028103018769436,
                "main_score": 40.396772024209824
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 30.722932078009418,
                "f1": 28.49491987141746,
                "main_score": 30.722932078009418
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 37.14189643577674,
                "f1": 32.52116385408168,
                "main_score": 37.14189643577674
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 8.214525891055818,
                "f1": 4.448399109965533,
                "main_score": 8.214525891055818
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 7.96570275722932,
                "f1": 5.128691464756114,
                "main_score": 7.96570275722932
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 21.55682582380632,
                "f1": 17.110218757379613,
                "main_score": 21.55682582380632
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 36.70141223940821,
                "f1": 32.96113533567822,
                "main_score": 36.70141223940821
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 28.80295897780767,
                "f1": 27.77008973951413,
                "main_score": 28.80295897780767
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 10.460659045057163,
                "f1": 7.90075042321315,
                "main_score": 10.460659045057163
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 27.720242098184265,
                "f1": 26.76341970948208,
                "main_score": 27.720242098184265
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 75.21183591123066,
                "f1": 74.55953469104787,
                "main_score": 75.21183591123066
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 71.52320107599192,
                "f1": 71.16094498697193,
                "main_score": 71.52320107599192
            }
        ]
    }
}