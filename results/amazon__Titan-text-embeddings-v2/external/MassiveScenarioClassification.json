{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 58.069939475453936,
                "f1": 55.04073616892449,
                "main_score": 58.069939475453936
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 38.214525891055814,
                "f1": 36.42184260742777,
                "main_score": 38.214525891055814
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 57.47141896435777,
                "f1": 57.22453431938479,
                "main_score": 57.47141896435777
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 54.37121721587089,
                "f1": 53.004976087120134,
                "main_score": 54.37121721587089
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 52.71687962340283,
                "f1": 51.140151342341646,
                "main_score": 52.71687962340283
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 49.502353732347004,
                "f1": 45.74604753969847,
                "main_score": 49.502353732347004
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 64.25689307330195,
                "f1": 62.25355539317913,
                "main_score": 64.25689307330195
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 71.27774041694688,
                "f1": 70.26880477280841,
                "main_score": 71.27774041694688
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 52.420981842636195,
                "f1": 50.824547366213565,
                "main_score": 52.420981842636195
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 74.11230665770006,
                "f1": 73.00723710263364,
                "main_score": 74.11230665770006
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 67.04102219233356,
                "f1": 66.7904194512351,
                "main_score": 67.04102219233356
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 60.1714862138534,
                "f1": 58.781208933846095,
                "main_score": 60.1714862138534
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 54.04841963685272,
                "f1": 51.185007148328545,
                "main_score": 54.04841963685272
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 69.76462676529927,
                "f1": 68.85227238388136,
                "main_score": 69.76462676529927
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 62.84801613987895,
                "f1": 61.18395865529196,
                "main_score": 62.84801613987895
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 62.17888365837256,
                "f1": 60.40570575783401,
                "main_score": 62.17888365837256
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 53.52051109616678,
                "f1": 51.210696278552014,
                "main_score": 53.52051109616678
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 45.94821788836584,
                "f1": 43.65062337089374,
                "main_score": 45.94821788836584
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 60.33288500336248,
                "f1": 59.50436947982156,
                "main_score": 60.33288500336248
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 50.09751176866174,
                "f1": 47.293838685239,
                "main_score": 50.09751176866174
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 66.49293880295897,
                "f1": 65.96586462307134,
                "main_score": 66.49293880295897
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 68.35911230665769,
                "f1": 67.77840431764355,
                "main_score": 68.35911230665769
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 50.585070611970416,
                "f1": 47.957277125670295,
                "main_score": 50.585070611970416
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 42.76059179556153,
                "f1": 40.446327361325565,
                "main_score": 42.76059179556153
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 40.648957632817755,
                "f1": 37.231284508608276,
                "main_score": 40.648957632817755
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 57.24613315400134,
                "f1": 55.14523425690653,
                "main_score": 57.24613315400134
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 63.839946200403496,
                "f1": 62.6239063060589,
                "main_score": 63.839946200403496
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 53.14391392064559,
                "f1": 50.08744471966442,
                "main_score": 53.14391392064559
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 58.8399462004035,
                "f1": 57.586991117740794,
                "main_score": 58.8399462004035
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 44.81842636180229,
                "f1": 42.82813975084655,
                "main_score": 44.81842636180229
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 58.90047074646939,
                "f1": 56.640503134745714,
                "main_score": 58.90047074646939
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 38.52051109616678,
                "f1": 36.504553927569454,
                "main_score": 38.52051109616678
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 64.63685272360458,
                "f1": 62.88129994502907,
                "main_score": 64.63685272360458
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 67.54203093476798,
                "f1": 66.02745142287087,
                "main_score": 67.54203093476798
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 64.00470746469402,
                "f1": 62.91845058355313,
                "main_score": 64.00470746469402
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 65.69939475453934,
                "f1": 65.37413822081011,
                "main_score": 65.69939475453934
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 57.19905850706121,
                "f1": 55.08271383695852,
                "main_score": 57.19905850706121
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 65.42367182246134,
                "f1": 64.61962307022019,
                "main_score": 65.42367182246134
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 55.147948890383326,
                "f1": 53.2933851469903,
                "main_score": 55.147948890383326
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 55.679219905850715,
                "f1": 52.80159603468007,
                "main_score": 55.679219905850715
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 69.42165433759246,
                "f1": 67.99984081248608,
                "main_score": 69.42165433759246
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 52.30329522528581,
                "f1": 50.10810382364662,
                "main_score": 52.30329522528581
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 56.186953597848024,
                "f1": 55.51656586643505,
                "main_score": 56.186953597848024
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 58.019502353732356,
                "f1": 56.260726586358736,
                "main_score": 58.019502353732356
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 52.55548083389374,
                "f1": 51.139712264362714,
                "main_score": 52.55548083389374
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 57.43443174176194,
                "f1": 55.76244076715635,
                "main_score": 57.43443174176194
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 61.55346334902488,
                "f1": 61.25819823057803,
                "main_score": 61.55346334902488
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 47.114996637525216,
                "f1": 45.20428169546973,
                "main_score": 47.114996637525216
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 56.83254875588434,
                "f1": 56.00919757601416,
                "main_score": 56.83254875588434
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 69.57969065232012,
                "f1": 69.17378512156806,
                "main_score": 69.57969065232012
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 64.02488231338263,
                "f1": 64.09790488949963,
                "main_score": 64.02488231338263
            }
        ]
    }
}