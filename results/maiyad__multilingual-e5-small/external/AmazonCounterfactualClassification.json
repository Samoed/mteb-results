{
    "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
    "task_name": "AmazonCounterfactualClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 73.79104477611939,
                "ap": 36.9996434842022,
                "f1": 67.95453679103099,
                "main_score": 73.79104477611939
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 71.64882226980728,
                "ap": 82.11942130026586,
                "f1": 69.87963421606715,
                "main_score": 71.64882226980728
            },
            {
                "hf_subset": "en-ext",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 75.8095952023988,
                "ap": 24.46869495579561,
                "f1": 63.00108480037597,
                "main_score": 75.8095952023988
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 64.186295503212,
                "ap": 15.496804690197042,
                "f1": 52.07153895475031,
                "main_score": 64.186295503212
            }
        ]
    }
}