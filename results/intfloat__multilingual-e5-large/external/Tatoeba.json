{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.89999999999999,
                "f1": 94.69999999999999,
                "precision": 94.11666666666667,
                "recall": 95.89999999999999,
                "main_score": 94.69999999999999
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 68.20809248554913,
                "f1": 63.431048720066066,
                "precision": 61.69143958161298,
                "recall": 68.20809248554913,
                "main_score": 63.431048720066066
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 71.21951219512195,
                "f1": 66.82926829268293,
                "precision": 65.1260162601626,
                "recall": 71.21951219512195,
                "main_score": 66.82926829268293
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.2,
                "f1": 96.26666666666667,
                "precision": 95.8,
                "recall": 97.2,
                "main_score": 96.26666666666667
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 99.3,
                "f1": 99.06666666666666,
                "precision": 98.95,
                "recall": 99.3,
                "main_score": 99.06666666666666
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.39999999999999,
                "f1": 96.63333333333333,
                "precision": 96.26666666666668,
                "recall": 97.39999999999999,
                "main_score": 96.63333333333333
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96,
                "f1": 94.86666666666666,
                "precision": 94.31666666666668,
                "recall": 96,
                "main_score": 94.86666666666666
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 47.01492537313433,
                "f1": 40.178867566927266,
                "precision": 38.179295828549556,
                "recall": 47.01492537313433,
                "main_score": 40.178867566927266
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 86.5,
                "f1": 83.62537480063796,
                "precision": 82.44555555555554,
                "recall": 86.5,
                "main_score": 83.62537480063796
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 80.48780487804879,
                "f1": 75.45644599303138,
                "precision": 73.37398373983739,
                "recall": 80.48780487804879,
                "main_score": 75.45644599303138
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.7,
                "f1": 91.95666666666666,
                "precision": 91.125,
                "recall": 93.7,
                "main_score": 91.95666666666666
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 91.73754556500607,
                "f1": 89.65168084244632,
                "precision": 88.73025516403402,
                "recall": 91.73754556500607,
                "main_score": 89.65168084244632
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 81.04347826086956,
                "f1": 76.2128364389234,
                "precision": 74.2,
                "recall": 81.04347826086956,
                "main_score": 76.2128364389234
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 83.65217391304348,
                "f1": 79.4376811594203,
                "precision": 77.65797101449274,
                "recall": 83.65217391304348,
                "main_score": 79.4376811594203
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 87.5,
                "f1": 85.02690476190476,
                "precision": 83.96261904761904,
                "recall": 87.5,
                "main_score": 85.02690476190476
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 89.3,
                "f1": 86.52333333333333,
                "precision": 85.22833333333332,
                "recall": 89.3,
                "main_score": 86.52333333333333
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 65.01809408926418,
                "f1": 59.00594446432805,
                "precision": 56.827215807915444,
                "recall": 65.01809408926418,
                "main_score": 59.00594446432805
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 91.2,
                "f1": 88.58,
                "precision": 87.33333333333334,
                "recall": 91.2,
                "main_score": 88.58
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 59.199999999999996,
                "f1": 53.299166276284915,
                "precision": 51.3383908045977,
                "recall": 59.199999999999996,
                "main_score": 53.299166276284915
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 93.2,
                "f1": 91.2,
                "precision": 90.25,
                "recall": 93.2,
                "main_score": 91.2
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 64.76190476190476,
                "f1": 59.867110667110666,
                "precision": 58.07390192653351,
                "recall": 64.76190476190476,
                "main_score": 59.867110667110666
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 76.2,
                "f1": 71.48147546897547,
                "precision": 69.65409090909091,
                "recall": 76.2,
                "main_score": 71.48147546897547
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 93.8,
                "f1": 92.14,
                "precision": 91.35833333333333,
                "recall": 93.8,
                "main_score": 92.14
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.89999999999999,
                "f1": 97.2,
                "precision": 96.85000000000001,
                "recall": 97.89999999999999,
                "main_score": 97.2
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 94.6,
                "f1": 92.93333333333334,
                "precision": 92.13333333333333,
                "recall": 94.6,
                "main_score": 92.93333333333334
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 74.1,
                "f1": 69.14817460317461,
                "precision": 67.2515873015873,
                "recall": 74.1,
                "main_score": 69.14817460317461
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 94.01333333333335,
                "precision": 93.46666666666667,
                "recall": 95.19999999999999,
                "main_score": 94.01333333333335
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 76.9,
                "f1": 72.07523809523809,
                "precision": 70.19777777777779,
                "recall": 76.9,
                "main_score": 72.07523809523809
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 94.1,
                "f1": 92.31666666666666,
                "precision": 91.43333333333332,
                "recall": 94.1,
                "main_score": 92.31666666666666
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.1,
                "precision": 96.76666666666668,
                "recall": 97.8,
                "main_score": 97.1
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 92.85714285714286,
                "f1": 90.92093441150045,
                "precision": 90.00449236298293,
                "recall": 92.85714285714286,
                "main_score": 90.92093441150045
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 93.16239316239316,
                "f1": 91.33903133903132,
                "precision": 90.56267806267806,
                "recall": 93.16239316239316,
                "main_score": 91.33903133903132
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.4,
                "f1": 90.25666666666666,
                "precision": 89.25833333333334,
                "recall": 92.4,
                "main_score": 90.25666666666666
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 90.22727272727272,
                "f1": 87.53030303030303,
                "precision": 86.37121212121211,
                "recall": 90.22727272727272,
                "main_score": 87.53030303030303
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 79.03563941299791,
                "f1": 74.7349505840072,
                "precision": 72.9035639412998,
                "recall": 79.03563941299791,
                "main_score": 74.7349505840072
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97,
                "f1": 96.15,
                "precision": 95.76666666666668,
                "recall": 97,
                "main_score": 96.15
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 76.26459143968872,
                "f1": 71.55642023346303,
                "precision": 69.7544932369835,
                "recall": 76.26459143968872,
                "main_score": 71.55642023346303
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 58.119658119658126,
                "f1": 51.65242165242165,
                "precision": 49.41768108434775,
                "recall": 58.119658119658126,
                "main_score": 51.65242165242165
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 74.3,
                "f1": 69.52055555555555,
                "precision": 67.7574938949939,
                "recall": 74.3,
                "main_score": 69.52055555555555
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 94.8,
                "f1": 93.31666666666666,
                "precision": 92.60000000000001,
                "recall": 94.8,
                "main_score": 93.31666666666666
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 76.63551401869158,
                "f1": 72.35202492211837,
                "precision": 70.60358255451713,
                "recall": 76.63551401869158,
                "main_score": 72.35202492211837
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.4,
                "f1": 88.4811111111111,
                "precision": 87.7452380952381,
                "recall": 90.4,
                "main_score": 88.4811111111111
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95,
                "f1": 93.60666666666667,
                "precision": 92.975,
                "recall": 95,
                "main_score": 93.60666666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 67.2,
                "f1": 63.01595782872099,
                "precision": 61.596587301587306,
                "recall": 67.2,
                "main_score": 63.01595782872099
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.7,
                "f1": 94.52999999999999,
                "precision": 94,
                "recall": 95.7,
                "main_score": 94.52999999999999
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.6,
                "f1": 93.28999999999999,
                "precision": 92.675,
                "recall": 94.6,
                "main_score": 93.28999999999999
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 96.39999999999999,
                "f1": 95.28333333333333,
                "precision": 94.75,
                "recall": 96.39999999999999,
                "main_score": 95.28333333333333
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 91.9,
                "f1": 89.83,
                "precision": 88.92,
                "recall": 91.9,
                "main_score": 89.83
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.69999999999999,
                "f1": 93.34222222222223,
                "precision": 92.75416666666668,
                "recall": 94.69999999999999,
                "main_score": 93.34222222222223
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 60.333333333333336,
                "f1": 55.31203703703703,
                "precision": 53.39971108326371,
                "recall": 60.333333333333336,
                "main_score": 55.31203703703703
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 12.9,
                "f1": 11.099861903031458,
                "precision": 10.589187932631877,
                "recall": 12.9,
                "main_score": 11.099861903031458
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 86.7,
                "f1": 83.0152380952381,
                "precision": 81.37833333333333,
                "recall": 86.7,
                "main_score": 83.0152380952381
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 63.39285714285714,
                "f1": 56.832482993197274,
                "precision": 54.56845238095237,
                "recall": 63.39285714285714,
                "main_score": 56.832482993197274
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 48.73765093304062,
                "f1": 41.555736920720456,
                "precision": 39.06874531737319,
                "recall": 48.73765093304062,
                "main_score": 41.555736920720456
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 41.099999999999994,
                "f1": 36.540165945165946,
                "precision": 35.05175685425686,
                "recall": 41.099999999999994,
                "main_score": 36.540165945165946
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.89999999999999,
                "f1": 93.42333333333333,
                "precision": 92.75833333333333,
                "recall": 94.89999999999999,
                "main_score": 93.42333333333333
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.89999999999999,
                "f1": 93.63333333333334,
                "precision": 93.01666666666665,
                "recall": 94.89999999999999,
                "main_score": 93.63333333333334
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 77.9,
                "f1": 73.64833333333334,
                "precision": 71.90282106782105,
                "recall": 77.9,
                "main_score": 73.64833333333334
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 59.4,
                "f1": 54.90521367521367,
                "precision": 53.432840025471606,
                "recall": 59.4,
                "main_score": 54.90521367521367
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.39999999999999,
                "f1": 96.6,
                "precision": 96.2,
                "recall": 97.39999999999999,
                "main_score": 96.6
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 67.2,
                "f1": 62.25926129426129,
                "precision": 60.408376623376626,
                "recall": 67.2,
                "main_score": 62.25926129426129
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.2,
                "f1": 87.60666666666667,
                "precision": 86.45277777777778,
                "recall": 90.2,
                "main_score": 87.60666666666667
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.7,
                "f1": 97,
                "precision": 96.65,
                "recall": 97.7,
                "main_score": 97
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.2,
                "f1": 91.39746031746031,
                "precision": 90.6125,
                "recall": 93.2,
                "main_score": 91.39746031746031
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 32.11678832116788,
                "f1": 27.210415386260234,
                "precision": 26.20408990846947,
                "recall": 32.11678832116788,
                "main_score": 27.210415386260234
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 8.5,
                "f1": 6.787319277832475,
                "precision": 6.3452094433344435,
                "recall": 8.5,
                "main_score": 6.787319277832475
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.1,
                "f1": 95.08,
                "precision": 94.61666666666667,
                "recall": 96.1,
                "main_score": 95.08
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 95.3,
                "f1": 93.88333333333333,
                "precision": 93.18333333333332,
                "recall": 95.3,
                "main_score": 93.88333333333333
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 85.11904761904762,
                "f1": 80.69444444444444,
                "precision": 78.72023809523809,
                "recall": 85.11904761904762,
                "main_score": 80.69444444444444
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.1,
                "f1": 9.276381801735853,
                "precision": 8.798174603174601,
                "recall": 11.1,
                "main_score": 9.276381801735853
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 63.56107660455487,
                "f1": 58.70433569191332,
                "precision": 56.896926581464015,
                "recall": 63.56107660455487,
                "main_score": 58.70433569191332
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 94.69999999999999,
                "f1": 93.10000000000001,
                "precision": 92.35,
                "recall": 94.69999999999999,
                "main_score": 93.10000000000001
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.8,
                "f1": 96.01222222222222,
                "precision": 95.67083333333332,
                "recall": 96.8,
                "main_score": 96.01222222222222
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 9.2,
                "f1": 7.911555250305249,
                "precision": 7.631246556216846,
                "recall": 9.2,
                "main_score": 7.911555250305249
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 77.48917748917748,
                "f1": 72.27375798804371,
                "precision": 70.14430014430013,
                "recall": 77.48917748917748,
                "main_score": 72.27375798804371
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 77.09923664122137,
                "f1": 72.61541257724463,
                "precision": 70.8998380754106,
                "recall": 77.09923664122137,
                "main_score": 72.61541257724463
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 98.2532751091703,
                "f1": 97.69529354682193,
                "precision": 97.42843279961184,
                "recall": 98.2532751091703,
                "main_score": 97.69529354682193
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 82.8,
                "f1": 79.14672619047619,
                "precision": 77.59489247311828,
                "recall": 82.8,
                "main_score": 79.14672619047619
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.35028248587571,
                "f1": 92.86252354048965,
                "precision": 92.2080979284369,
                "recall": 94.35028248587571,
                "main_score": 92.86252354048965
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 8.5,
                "f1": 6.282429263935621,
                "precision": 5.783274240739785,
                "recall": 8.5,
                "main_score": 6.282429263935621
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.7,
                "f1": 91.025,
                "precision": 90.30428571428571,
                "recall": 92.7,
                "main_score": 91.025
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 81,
                "f1": 77.8232380952381,
                "precision": 76.60194444444444,
                "recall": 81,
                "main_score": 77.8232380952381
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 91,
                "f1": 88.70857142857142,
                "precision": 87.7,
                "recall": 91,
                "main_score": 88.70857142857142
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.39999999999999,
                "f1": 95.3,
                "precision": 94.76666666666667,
                "recall": 96.39999999999999,
                "main_score": 95.3
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 8.1,
                "f1": 7.001008218834307,
                "precision": 6.708329562594269,
                "recall": 8.1,
                "main_score": 7.001008218834307
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 87.1313672922252,
                "f1": 84.09070598748882,
                "precision": 82.79171454104429,
                "recall": 87.1313672922252,
                "main_score": 84.09070598748882
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 96.39999999999999,
                "f1": 95.28333333333333,
                "precision": 94.73333333333332,
                "recall": 96.39999999999999,
                "main_score": 95.28333333333333
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 42.29249011857708,
                "f1": 36.981018542283365,
                "precision": 35.415877813576024,
                "recall": 42.29249011857708,
                "main_score": 36.981018542283365
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 83.80281690140845,
                "f1": 80.86854460093896,
                "precision": 79.60093896713614,
                "recall": 83.80281690140845,
                "main_score": 80.86854460093896
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 45.26946107784431,
                "f1": 39.80235464678088,
                "precision": 38.14342660001342,
                "recall": 45.26946107784431,
                "main_score": 39.80235464678088
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.3,
                "f1": 92.9,
                "precision": 92.26666666666668,
                "recall": 94.3,
                "main_score": 92.9
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 37.93103448275862,
                "f1": 33.15192743764172,
                "precision": 31.57456528146183,
                "recall": 37.93103448275862,
                "main_score": 33.15192743764172
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 69.01408450704226,
                "f1": 63.41549295774648,
                "precision": 61.342778895595806,
                "recall": 69.01408450704226,
                "main_score": 63.41549295774648
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 76.66666666666667,
                "f1": 71.60705960705961,
                "precision": 69.60683760683762,
                "recall": 76.66666666666667,
                "main_score": 71.60705960705961
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 95.8,
                "f1": 94.48333333333333,
                "precision": 93.83333333333333,
                "recall": 95.8,
                "main_score": 94.48333333333333
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 52.81837160751566,
                "f1": 48.435977731384824,
                "precision": 47.11291973845539,
                "recall": 52.81837160751566,
                "main_score": 48.435977731384824
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 44.9,
                "f1": 38.88962621607783,
                "precision": 36.95936507936508,
                "recall": 44.9,
                "main_score": 38.88962621607783
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 90.55374592833876,
                "f1": 88.22553125484721,
                "precision": 87.26927252985884,
                "recall": 90.55374592833876,
                "main_score": 88.22553125484721
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.6,
                "f1": 93.13333333333333,
                "precision": 92.45333333333333,
                "recall": 94.6,
                "main_score": 93.13333333333333
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.7,
                "f1": 91.99666666666667,
                "precision": 91.26666666666668,
                "recall": 93.7,
                "main_score": 91.99666666666667
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 85.03937007874016,
                "f1": 81.75853018372703,
                "precision": 80.34120734908137,
                "recall": 85.03937007874016,
                "main_score": 81.75853018372703
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 88.3,
                "f1": 85.5,
                "precision": 84.25833333333334,
                "recall": 88.3,
                "main_score": 85.5
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 65.51246537396122,
                "f1": 60.02297410192148,
                "precision": 58.133467727289236,
                "recall": 65.51246537396122,
                "main_score": 60.02297410192148
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96,
                "f1": 94.89,
                "precision": 94.39166666666667,
                "recall": 96,
                "main_score": 94.89
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 57.692307692307686,
                "f1": 53.162393162393165,
                "precision": 51.70673076923077,
                "recall": 57.692307692307686,
                "main_score": 53.162393162393165
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 91.60000000000001,
                "f1": 89.21190476190475,
                "precision": 88.08666666666667,
                "recall": 91.60000000000001,
                "main_score": 89.21190476190475
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 88,
                "f1": 85.47,
                "precision": 84.43266233766234,
                "recall": 88,
                "main_score": 85.47
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 92.7,
                "f1": 90.64999999999999,
                "precision": 89.68333333333332,
                "recall": 92.7,
                "main_score": 90.64999999999999
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 80.30660377358491,
                "f1": 76.33044137466307,
                "precision": 74.78970125786164,
                "recall": 80.30660377358491,
                "main_score": 76.33044137466307
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.39999999999999,
                "f1": 95.44,
                "precision": 94.99166666666666,
                "recall": 96.39999999999999,
                "main_score": 95.44
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 96.53284671532847,
                "f1": 95.37712895377129,
                "precision": 94.7992700729927,
                "recall": 96.53284671532847,
                "main_score": 95.37712895377129
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 89,
                "f1": 86.23190476190476,
                "precision": 85.035,
                "recall": 89,
                "main_score": 86.23190476190476
            }
        ]
    }
}