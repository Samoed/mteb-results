{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 68.73907195696032,
                "f1": 66.98484521791418,
                "main_score": 68.73907195696032
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 60.58843308675185,
                "f1": 58.95591723092005,
                "main_score": 60.58843308675185
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 66.22730329522528,
                "f1": 66.0894499712115,
                "main_score": 66.22730329522528
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 66.48285137861465,
                "f1": 65.21963176785157,
                "main_score": 66.48285137861465
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 67.74714189643578,
                "f1": 66.8212192745412,
                "main_score": 67.74714189643578
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 59.09213180901143,
                "f1": 56.70735546356339,
                "main_score": 59.09213180901143
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 75.05716207128448,
                "f1": 74.8413712365364,
                "main_score": 75.05716207128448
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 74.69737726967047,
                "f1": 74.7664341963,
                "main_score": 74.69737726967047
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 73.90383322125084,
                "f1": 73.59201554448323,
                "main_score": 73.90383322125084
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 77.51176866173503,
                "f1": 77.46104434577758,
                "main_score": 77.51176866173503
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 74.31069266980496,
                "f1": 74.61048660675635,
                "main_score": 74.31069266980496
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 72.95225285810356,
                "f1": 72.33160006574627,
                "main_score": 72.95225285810356
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 73.12373907195696,
                "f1": 73.20921012557481,
                "main_score": 73.12373907195696
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 73.86684599865501,
                "f1": 73.82348774610831,
                "main_score": 73.86684599865501
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 71.40215198386012,
                "f1": 71.11945183971858,
                "main_score": 71.40215198386012
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 72.12844653665098,
                "f1": 71.34450495911766,
                "main_score": 72.12844653665098
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 74.52252858103566,
                "f1": 73.98878711342999,
                "main_score": 74.52252858103566
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 64.93611297915265,
                "f1": 63.723200467653385,
                "main_score": 64.93611297915265
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 74.11903160726295,
                "f1": 73.82138439467096,
                "main_score": 74.11903160726295
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 67.15198386012105,
                "f1": 66.02172193802167,
                "main_score": 67.15198386012105
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 74.32414256893072,
                "f1": 74.30943421170574,
                "main_score": 74.32414256893072
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 77.46805648957633,
                "f1": 77.62808409298209,
                "main_score": 77.46805648957633
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 63.318762609280434,
                "f1": 62.094284066075076,
                "main_score": 63.318762609280434
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 58.34902488231338,
                "f1": 57.12893860987984,
                "main_score": 58.34902488231338
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 50.88433086751849,
                "f1": 48.2272350802058,
                "main_score": 50.88433086751849
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 66.4425016812374,
                "f1": 64.61463095996173,
                "main_score": 66.4425016812374
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 75.04707464694015,
                "f1": 75.05099199098998,
                "main_score": 75.04707464694015
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 70.50437121721586,
                "f1": 69.83397721096314,
                "main_score": 70.50437121721586
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 69.94283792871553,
                "f1": 68.8704663703913,
                "main_score": 69.94283792871553
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 64.79488903833222,
                "f1": 63.615424063345436,
                "main_score": 64.79488903833222
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 69.88231338264963,
                "f1": 68.57892302593237,
                "main_score": 69.88231338264963
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 63.248150638870214,
                "f1": 61.06680605338809,
                "main_score": 63.248150638870214
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 74.84196368527236,
                "f1": 74.52566464968763,
                "main_score": 74.84196368527236
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 74.8285137861466,
                "f1": 74.8853197608802,
                "main_score": 74.8285137861466
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 74.13248150638869,
                "f1": 74.3982040999179,
                "main_score": 74.13248150638869
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 73.49024882313383,
                "f1": 73.82153848368573,
                "main_score": 73.49024882313383
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 71.72158708809684,
                "f1": 71.85049433180541,
                "main_score": 71.72158708809684
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 75.137861466039,
                "f1": 75.37628348188467,
                "main_score": 75.137861466039
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 71.86953597848016,
                "f1": 71.87537624521661,
                "main_score": 71.86953597848016
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 70.27572293207801,
                "f1": 68.80017302344231,
                "main_score": 70.27572293207801
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 76.09952925353059,
                "f1": 76.07992707688408,
                "main_score": 76.09952925353059
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 63.140551445864155,
                "f1": 61.73855010331415,
                "main_score": 63.140551445864155
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 66.27774041694687,
                "f1": 64.83664868894539,
                "main_score": 66.27774041694687
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 66.69468728984533,
                "f1": 64.76239666920868,
                "main_score": 66.69468728984533
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 73.44653665097512,
                "f1": 73.14646052013873,
                "main_score": 73.44653665097512
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 67.71351714862139,
                "f1": 66.67212180163382,
                "main_score": 67.71351714862139
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 73.9946200403497,
                "f1": 73.87348793725525,
                "main_score": 73.9946200403497
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 68.15400134498992,
                "f1": 67.09433241421094,
                "main_score": 68.15400134498992
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 73.11365164761264,
                "f1": 73.59502539433753,
                "main_score": 73.11365164761264
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 76.82582380632145,
                "f1": 76.89992945316313,
                "main_score": 76.82582380632145
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 71.81237390719569,
                "f1": 72.36499770986265,
                "main_score": 71.81237390719569
            }
        ]
    }
}