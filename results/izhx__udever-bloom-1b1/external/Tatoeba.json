{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 16.0,
                "f1": 12.072197229668266,
                "precision": 11.07125213426268,
                "recall": 16.0,
                "main_score": 12.072197229668266
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 31.79190751445087,
                "f1": 25.33993944398569,
                "precision": 23.462449892587426,
                "recall": 31.79190751445087,
                "main_score": 25.33993944398569
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 14.390243902439023,
                "f1": 10.647146321087272,
                "precision": 9.753700307679768,
                "recall": 14.390243902439023,
                "main_score": 10.647146321087272
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 7.8,
                "f1": 5.087296515623526,
                "precision": 4.543963123070674,
                "recall": 7.8,
                "main_score": 5.087296515623526
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 58.5,
                "f1": 53.26571428571428,
                "precision": 51.32397398353281,
                "recall": 58.5,
                "main_score": 53.26571428571428
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 29.5,
                "f1": 25.14837668933257,
                "precision": 23.949224030449837,
                "recall": 29.5,
                "main_score": 25.14837668933257
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 28.7,
                "f1": 23.196045369663018,
                "precision": 21.502155293536873,
                "recall": 28.7,
                "main_score": 23.196045369663018
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 27.611940298507463,
                "f1": 19.431414356787492,
                "precision": 17.160948504232085,
                "recall": 27.611940298507463,
                "main_score": 19.431414356787492
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 46.0,
                "f1": 39.146820760938404,
                "precision": 36.89055652165172,
                "recall": 46.0,
                "main_score": 39.146820760938404
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 23.414634146341466,
                "f1": 18.60234074868221,
                "precision": 17.310239781020474,
                "recall": 23.414634146341466,
                "main_score": 18.60234074868221
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 7.3,
                "f1": 5.456411432480631,
                "precision": 5.073425278627456,
                "recall": 7.3,
                "main_score": 5.456411432480631
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 10.814094775212636,
                "f1": 8.096556306772158,
                "precision": 7.501928709802902,
                "recall": 10.814094775212636,
                "main_score": 8.096556306772158
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.304347826086957,
                "f1": 7.766717493033283,
                "precision": 6.980930791147511,
                "recall": 11.304347826086957,
                "main_score": 7.766717493033283
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 6.260869565217392,
                "f1": 4.695624631925284,
                "precision": 4.520242639508398,
                "recall": 6.260869565217392,
                "main_score": 4.695624631925284
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 6.9,
                "f1": 4.467212205066257,
                "precision": 4.004142723685108,
                "recall": 6.9,
                "main_score": 4.467212205066257
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 1.0999999999999999,
                "f1": 0.6945869191049914,
                "precision": 0.6078431372549019,
                "recall": 1.0999999999999999,
                "main_score": 0.6945869191049914
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 4.583835946924005,
                "f1": 2.9858475730729075,
                "precision": 2.665996515212438,
                "recall": 4.583835946924005,
                "main_score": 2.9858475730729075
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 59.199999999999996,
                "f1": 52.67345238095238,
                "precision": 50.13575757575758,
                "recall": 59.199999999999996,
                "main_score": 52.67345238095238
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 35.0,
                "f1": 27.648653013653007,
                "precision": 25.534839833369244,
                "recall": 35.0,
                "main_score": 27.648653013653007
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 13.100000000000001,
                "f1": 9.62336638477808,
                "precision": 8.875194920058407,
                "recall": 13.100000000000001,
                "main_score": 9.62336638477808
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 32.95238095238095,
                "f1": 27.600581429152854,
                "precision": 26.078624096473064,
                "recall": 32.95238095238095,
                "main_score": 27.600581429152854
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 6.5,
                "f1": 3.9595645184317045,
                "precision": 3.5893378968989453,
                "recall": 6.5,
                "main_score": 3.9595645184317045
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 17.8,
                "f1": 13.508124743694003,
                "precision": 12.24545634920635,
                "recall": 17.8,
                "main_score": 13.508124743694003
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 21.7,
                "f1": 17.67074499610417,
                "precision": 16.47070885787265,
                "recall": 21.7,
                "main_score": 17.67074499610417
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 19.3,
                "f1": 14.249803276788573,
                "precision": 12.916981621996223,
                "recall": 19.3,
                "main_score": 14.249803276788573
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 67.2,
                "f1": 61.03507936507936,
                "precision": 58.69699346405229,
                "recall": 67.2,
                "main_score": 61.03507936507936
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 6.5,
                "f1": 4.295097572176196,
                "precision": 3.809609027256814,
                "recall": 6.5,
                "main_score": 4.295097572176196
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 2.8000000000000003,
                "f1": 1.678577135635959,
                "precision": 1.455966810966811,
                "recall": 2.8000000000000003,
                "main_score": 1.678577135635959
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 47.9,
                "f1": 40.26661017143776,
                "precision": 37.680778943278945,
                "recall": 47.9,
                "main_score": 40.26661017143776
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.0,
                "f1": 96.05,
                "precision": 95.58333333333334,
                "recall": 97.0,
                "main_score": 96.05
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.9433962264150944,
                "f1": 0.6457074216068709,
                "precision": 0.6068362258275373,
                "recall": 0.9433962264150944,
                "main_score": 0.6457074216068709
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 74.78632478632478,
                "f1": 69.05372405372405,
                "precision": 66.82336182336182,
                "recall": 74.78632478632478,
                "main_score": 69.05372405372405
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 19.2,
                "f1": 14.54460169057995,
                "precision": 13.265236397589335,
                "recall": 19.2,
                "main_score": 14.54460169057995
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 6.8181818181818175,
                "f1": 4.78808236251355,
                "precision": 4.4579691142191145,
                "recall": 6.8181818181818175,
                "main_score": 4.78808236251355
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 72.53668763102725,
                "f1": 66.00978336827393,
                "precision": 63.21104122990915,
                "recall": 72.53668763102725,
                "main_score": 66.00978336827393
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 12.7,
                "f1": 9.731576351893512,
                "precision": 8.986658245110663,
                "recall": 12.7,
                "main_score": 9.731576351893512
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 57.19844357976653,
                "f1": 49.138410227904394,
                "precision": 45.88197146562906,
                "recall": 57.19844357976653,
                "main_score": 49.138410227904394
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 28.205128205128204,
                "f1": 21.863766936230704,
                "precision": 20.212164378831048,
                "recall": 28.205128205128204,
                "main_score": 21.863766936230704
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 23.3,
                "f1": 17.75959261382939,
                "precision": 16.18907864830205,
                "recall": 23.3,
                "main_score": 17.75959261382939
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 19.1,
                "f1": 14.320618913993744,
                "precision": 12.980748202777615,
                "recall": 19.1,
                "main_score": 14.320618913993744
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 8.411214953271028,
                "f1": 5.152309182683014,
                "precision": 4.456214003721122,
                "recall": 8.411214953271028,
                "main_score": 5.152309182683014
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 6.7,
                "f1": 4.833930504764646,
                "precision": 4.475394510103751,
                "recall": 6.7,
                "main_score": 4.833930504764646
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 79.4,
                "f1": 74.59166666666667,
                "precision": 72.59928571428571,
                "recall": 79.4,
                "main_score": 74.59166666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 47.8,
                "f1": 41.944877899877895,
                "precision": 39.87211701696996,
                "recall": 47.8,
                "main_score": 41.944877899877895
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 85.0,
                "f1": 81.47666666666666,
                "precision": 79.95909090909092,
                "recall": 85.0,
                "main_score": 81.47666666666666
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 62.6,
                "f1": 55.96755336167101,
                "precision": 53.49577131202131,
                "recall": 62.6,
                "main_score": 55.96755336167101
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 95.3,
                "f1": 93.96666666666668,
                "precision": 93.33333333333333,
                "recall": 95.3,
                "main_score": 93.96666666666668
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 7.7,
                "f1": 5.534253062728994,
                "precision": 4.985756669800788,
                "recall": 7.7,
                "main_score": 5.534253062728994
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 80.5,
                "f1": 75.91705128205129,
                "precision": 73.96261904761904,
                "recall": 80.5,
                "main_score": 75.91705128205129
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 10.333333333333334,
                "f1": 7.753678057001793,
                "precision": 7.207614225986279,
                "recall": 10.333333333333334,
                "main_score": 7.753678057001793
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 8.6,
                "f1": 5.345683110450071,
                "precision": 4.569931461907268,
                "recall": 8.6,
                "main_score": 5.345683110450071
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 82.8,
                "f1": 78.75999999999999,
                "precision": 76.97666666666666,
                "recall": 82.8,
                "main_score": 78.75999999999999
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 26.785714285714285,
                "f1": 21.62627551020408,
                "precision": 20.17219387755102,
                "recall": 26.785714285714285,
                "main_score": 21.62627551020408
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 32.93084522502745,
                "f1": 26.281513627941628,
                "precision": 24.05050619189897,
                "recall": 32.93084522502745,
                "main_score": 26.281513627941628
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 2.1,
                "f1": 1.144678201129814,
                "precision": 1.0228433014856975,
                "recall": 2.1,
                "main_score": 1.144678201129814
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.3,
                "f1": 92.77000000000001,
                "precision": 92.09166666666667,
                "recall": 94.3,
                "main_score": 92.77000000000001
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.1,
                "f1": 92.51666666666667,
                "precision": 91.75,
                "recall": 94.1,
                "main_score": 92.51666666666667
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 4.1000000000000005,
                "f1": 2.856566814643248,
                "precision": 2.6200368188362506,
                "recall": 4.1000000000000005,
                "main_score": 2.856566814643248
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 45.9,
                "f1": 39.02207792207792,
                "precision": 36.524158064158065,
                "recall": 45.9,
                "main_score": 39.02207792207792
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 13.4,
                "f1": 9.61091517529598,
                "precision": 8.755127233877234,
                "recall": 13.4,
                "main_score": 9.61091517529598
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.1,
                "f1": 8.068379205189386,
                "precision": 7.400827352459544,
                "recall": 11.1,
                "main_score": 8.068379205189386
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 8.9,
                "f1": 6.632376174517077,
                "precision": 6.07114926880766,
                "recall": 8.9,
                "main_score": 6.632376174517077
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.8,
                "f1": 94.57333333333334,
                "precision": 93.99166666666667,
                "recall": 95.8,
                "main_score": 94.57333333333334
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 16.6,
                "f1": 13.328940031174618,
                "precision": 12.47204179664362,
                "recall": 16.6,
                "main_score": 13.328940031174618
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 29.927007299270077,
                "f1": 22.899432278994322,
                "precision": 20.917701519891303,
                "recall": 29.927007299270077,
                "main_score": 22.899432278994322
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 3.5000000000000004,
                "f1": 2.3809722674927083,
                "precision": 2.1368238705738705,
                "recall": 3.5000000000000004,
                "main_score": 2.3809722674927083
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 21.6,
                "f1": 17.54705304666238,
                "precision": 16.40586970344022,
                "recall": 21.6,
                "main_score": 17.54705304666238
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 3.5999999999999996,
                "f1": 2.3374438522182763,
                "precision": 2.099034070054354,
                "recall": 3.5999999999999996,
                "main_score": 2.3374438522182763
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 1.7857142857142856,
                "f1": 0.12056962540054328,
                "precision": 0.0628414244485673,
                "recall": 1.7857142857142856,
                "main_score": 0.12056962540054328
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 7.3999999999999995,
                "f1": 5.677284679983816,
                "precision": 5.314304945764335,
                "recall": 7.3999999999999995,
                "main_score": 5.677284679983816
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 13.043478260869565,
                "f1": 9.776306477806768,
                "precision": 9.09389484497104,
                "recall": 13.043478260869565,
                "main_score": 9.776306477806768
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 12.3,
                "f1": 8.757454269574472,
                "precision": 7.882868657107786,
                "recall": 12.3,
                "main_score": 8.757454269574472
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 28.9,
                "f1": 23.108557220070377,
                "precision": 21.35433328562513,
                "recall": 28.9,
                "main_score": 23.108557220070377
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 6.4,
                "f1": 4.781499273475174,
                "precision": 4.4496040053464565,
                "recall": 6.4,
                "main_score": 4.781499273475174
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 51.94805194805194,
                "f1": 45.658020784071205,
                "precision": 43.54163933709388,
                "recall": 51.94805194805194,
                "main_score": 45.658020784071205
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 14.50381679389313,
                "f1": 9.416337348733041,
                "precision": 8.17070085031468,
                "recall": 14.50381679389313,
                "main_score": 9.416337348733041
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 88.79184861717613,
                "f1": 85.56040756914118,
                "precision": 84.08539543910723,
                "recall": 88.79184861717613,
                "main_score": 85.56040756914118
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 62.5,
                "f1": 56.0802331002331,
                "precision": 53.613788230739445,
                "recall": 62.5,
                "main_score": 56.0802331002331
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 16.101694915254235,
                "f1": 11.927172795816864,
                "precision": 10.939011968423735,
                "recall": 16.101694915254235,
                "main_score": 11.927172795816864
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 5.5,
                "f1": 3.1258727724517197,
                "precision": 2.679506580565404,
                "recall": 5.5,
                "main_score": 3.1258727724517197
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 87.6,
                "f1": 84.53666666666666,
                "precision": 83.125,
                "recall": 87.6,
                "main_score": 84.53666666666666
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 65.7,
                "f1": 59.64428571428571,
                "precision": 57.30171568627451,
                "recall": 65.7,
                "main_score": 59.64428571428571
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 84.7,
                "f1": 81.34523809523809,
                "precision": 79.82777777777778,
                "recall": 84.7,
                "main_score": 81.34523809523809
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 18.6,
                "f1": 14.93884103295868,
                "precision": 14.059478087803882,
                "recall": 18.6,
                "main_score": 14.93884103295868
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 5.5,
                "f1": 3.815842342611909,
                "precision": 3.565130046415928,
                "recall": 5.5,
                "main_score": 3.815842342611909
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 1.2064343163538873,
                "f1": 0.9147778048582338,
                "precision": 0.8441848589301671,
                "recall": 1.2064343163538873,
                "main_score": 0.9147778048582338
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 71.3,
                "f1": 65.97350649350648,
                "precision": 63.85277777777777,
                "recall": 71.3,
                "main_score": 65.97350649350648
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 13.043478260869565,
                "f1": 9.043759194508343,
                "precision": 8.097993164155737,
                "recall": 13.043478260869565,
                "main_score": 9.043759194508343
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.267605633802818,
                "f1": 8.30172606520348,
                "precision": 7.737059013603729,
                "recall": 11.267605633802818,
                "main_score": 8.30172606520348
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 5.029940119760479,
                "f1": 3.07264903262435,
                "precision": 2.7633481831401783,
                "recall": 5.029940119760479,
                "main_score": 3.07264903262435
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.60000000000001,
                "f1": 88.29666666666667,
                "precision": 87.21666666666667,
                "recall": 90.60000000000001,
                "main_score": 88.29666666666667
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 7.389162561576355,
                "f1": 5.142049156827481,
                "precision": 4.756506859714838,
                "recall": 7.389162561576355,
                "main_score": 5.142049156827481
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 44.36619718309859,
                "f1": 39.378676538811256,
                "precision": 37.71007182068377,
                "recall": 44.36619718309859,
                "main_score": 39.378676538811256
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 21.794871794871796,
                "f1": 16.314588577641768,
                "precision": 14.962288221599962,
                "recall": 21.794871794871796,
                "main_score": 16.314588577641768
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 93.5,
                "f1": 91.53333333333333,
                "precision": 90.58333333333333,
                "recall": 93.5,
                "main_score": 91.53333333333333
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 12.526096033402922,
                "f1": 9.57488704957882,
                "precision": 8.943001322776725,
                "recall": 12.526096033402922,
                "main_score": 9.57488704957882
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 6.9,
                "f1": 4.5770099528158,
                "precision": 4.166915172638407,
                "recall": 6.9,
                "main_score": 4.5770099528158
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 81.75895765472313,
                "f1": 77.29641693811075,
                "precision": 75.3528773072747,
                "recall": 81.75895765472313,
                "main_score": 77.29641693811075
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.0,
                "f1": 8.522094712720397,
                "precision": 7.883076528738328,
                "recall": 11.0,
                "main_score": 8.522094712720397
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.3,
                "f1": 8.626190704312432,
                "precision": 7.994434420637179,
                "recall": 11.3,
                "main_score": 8.626190704312432
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 74.01574803149606,
                "f1": 68.16272965879266,
                "precision": 65.99737532808399,
                "recall": 74.01574803149606,
                "main_score": 68.16272965879266
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 9.0,
                "f1": 6.189958106409719,
                "precision": 5.445330404889228,
                "recall": 9.0,
                "main_score": 6.189958106409719
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.2770083102493075,
                "f1": 0.011664800298618888,
                "precision": 0.005957856811560036,
                "recall": 0.2770083102493075,
                "main_score": 0.011664800298618888
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 8.799999999999999,
                "f1": 5.636139438882621,
                "precision": 4.993972914553003,
                "recall": 8.799999999999999,
                "main_score": 5.636139438882621
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 37.5,
                "f1": 31.31118881118881,
                "precision": 29.439102564102566,
                "recall": 37.5,
                "main_score": 31.31118881118881
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 74.5,
                "f1": 68.96380952380953,
                "precision": 66.67968253968255,
                "recall": 74.5,
                "main_score": 68.96380952380953
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 89.0,
                "f1": 86.42523809523809,
                "precision": 85.28333333333332,
                "recall": 89.0,
                "main_score": 86.42523809523809
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 17.2,
                "f1": 12.555081585081584,
                "precision": 11.292745310245309,
                "recall": 17.2,
                "main_score": 12.555081585081584
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.3537735849056604,
                "f1": 0.12010530448397783,
                "precision": 0.11902214818132154,
                "recall": 0.3537735849056604,
                "main_score": 0.12010530448397783
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 5.8999999999999995,
                "f1": 4.26942162679512,
                "precision": 3.967144120536608,
                "recall": 5.8999999999999995,
                "main_score": 4.26942162679512
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 2.737226277372263,
                "f1": 1.64474042578532,
                "precision": 1.567547886228932,
                "recall": 2.737226277372263,
                "main_score": 1.64474042578532
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 84.89999999999999,
                "f1": 81.17555555555555,
                "precision": 79.56416666666667,
                "recall": 84.89999999999999,
                "main_score": 81.17555555555555
            }
        ]
    }
}