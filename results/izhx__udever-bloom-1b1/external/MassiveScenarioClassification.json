{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": NaN,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 50.46738399462004,
                "f1": 48.277337351043066,
                "main_score": 50.46738399462004
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 27.222595830531272,
                "f1": 26.15959037949326,
                "main_score": 27.222595830531272
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 65.4303967720242,
                "f1": 65.58227814316872,
                "main_score": 65.4303967720242
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 40.736381977135174,
                "f1": 39.85702036251076,
                "main_score": 40.736381977135174
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 67.64626765299259,
                "f1": 67.12298813657769,
                "main_score": 67.64626765299259
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 43.940820443846675,
                "f1": 41.63412499587839,
                "main_score": 43.940820443846675
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 52.5252185608608,
                "f1": 50.25821961669483,
                "main_score": 52.5252185608608
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 56.67114996637525,
                "f1": 54.204117831814244,
                "main_score": 56.67114996637525
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 41.8123739071957,
                "f1": 40.25676895490678,
                "main_score": 41.8123739071957
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 75.71956960322798,
                "f1": 75.95126212201126,
                "main_score": 75.71956960322798
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 71.7787491593813,
                "f1": 71.90678548502461,
                "main_score": 71.7787491593813
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 49.95965030262274,
                "f1": 48.625859921623515,
                "main_score": 49.95965030262274
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 41.005379959650305,
                "f1": 38.25957953711836,
                "main_score": 41.005379959650305
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 71.99058507061198,
                "f1": 72.30034867942928,
                "main_score": 71.99058507061198
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 36.691324815063886,
                "f1": 35.09762112518494,
                "main_score": 36.691324815063886
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 69.27706792199058,
                "f1": 68.96935505580095,
                "main_score": 69.27706792199058
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 44.31405514458642,
                "f1": 41.75837557089336,
                "main_score": 44.31405514458642
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 33.63819771351715,
                "f1": 32.00999199645466,
                "main_score": 33.63819771351715
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 68.98117014122394,
                "f1": 68.48993356947226,
                "main_score": 68.98117014122394
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 42.10154673839946,
                "f1": 39.537580201439035,
                "main_score": 42.10154673839946
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 60.27236045729657,
                "f1": 58.8041857941664,
                "main_score": 60.27236045729657
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 62.47814391392063,
                "f1": 61.4800551358116,
                "main_score": 62.47814391392063
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 54.68392737054473,
                "f1": 53.28619831432411,
                "main_score": 54.68392737054473
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 27.215870880968396,
                "f1": 26.137784395348483,
                "main_score": 27.215870880968396
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 32.1385339609953,
                "f1": 29.886918185071977,
                "main_score": 32.1385339609953
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 57.94889038332213,
                "f1": 57.19252000109654,
                "main_score": 57.94889038332213
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 47.94552790854068,
                "f1": 46.21337507975437,
                "main_score": 47.94552790854068
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 42.75722932078009,
                "f1": 40.62195245815035,
                "main_score": 42.75722932078009
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 62.84129119031607,
                "f1": 62.56205475932971,
                "main_score": 62.84129119031607
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 33.21116341627438,
                "f1": 32.231827617771046,
                "main_score": 33.21116341627438
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 62.56893073301949,
                "f1": 60.94616552257348,
                "main_score": 62.56893073301949
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 28.8399462004035,
                "f1": 27.8503615081592,
                "main_score": 28.8399462004035
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 50.31607262945528,
                "f1": 47.993368005418205,
                "main_score": 50.31607262945528
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 52.851378614660405,
                "f1": 50.444332639513824,
                "main_score": 52.851378614660405
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 45.595158036314736,
                "f1": 44.241686886064755,
                "main_score": 45.595158036314736
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 70.24209818426363,
                "f1": 70.48109122752663,
                "main_score": 70.24209818426363
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 52.73369199731002,
                "f1": 51.14034087602817,
                "main_score": 52.73369199731002
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 54.263618022864826,
                "f1": 53.3188846615122,
                "main_score": 54.263618022864826
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 46.88634835238735,
                "f1": 45.257261686960796,
                "main_score": 46.88634835238735
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 47.15534633490249,
                "f1": 45.218807618409215,
                "main_score": 47.15534633490249
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 47.9119031607263,
                "f1": 45.96730030717468,
                "main_score": 47.9119031607263
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 51.20040349697377,
                "f1": 49.113423730259214,
                "main_score": 51.20040349697377
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 61.8392737054472,
                "f1": 61.65834459536364,
                "main_score": 61.8392737054472
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 59.791526563550775,
                "f1": 58.2891677685128,
                "main_score": 59.791526563550775
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 41.62071284465366,
                "f1": 39.591525429243575,
                "main_score": 41.62071284465366
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 50.46738399462004,
                "f1": 49.50612154409957,
                "main_score": 50.46738399462004
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 43.41291190316072,
                "f1": 43.85070302174815,
                "main_score": 43.41291190316072
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 60.15131136516476,
                "f1": 59.260012738676316,
                "main_score": 60.15131136516476
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 68.98789509078682,
                "f1": 69.86968024553558,
                "main_score": 68.98789509078682
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 74.72091459314055,
                "f1": 74.69866015852224,
                "main_score": 74.72091459314055
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 71.7014122394082,
                "f1": 72.66856729607628,
                "main_score": 71.7014122394082
            }
        ]
    }
}