{
    "dataset_revision": "31efe3c427b0bae9c22cbb560b8f15491cc6bed7",
    "task_name": "MassiveIntentClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 57.06792199058508,
                "f1": 54.094921857502285,
                "main_score": 57.06792199058508
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 51.960322797579025,
                "f1": 48.547371223370945,
                "main_score": 51.960322797579025
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 54.425016812373904,
                "f1": 50.47069202054312,
                "main_score": 54.425016812373904
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 59.798251513113655,
                "f1": 57.05013069086648,
                "main_score": 59.798251513113655
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 59.37794216543376,
                "f1": 56.3607992649805,
                "main_score": 59.37794216543376
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 46.56018829858777,
                "f1": 43.87319715715134,
                "main_score": 46.56018829858777
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 62.9724277067922,
                "f1": 59.36480066245562,
                "main_score": 62.9724277067922
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 62.72696704774715,
                "f1": 59.143595966615855,
                "main_score": 62.72696704774715
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 61.5971755211836,
                "f1": 59.169445724946726,
                "main_score": 61.5971755211836
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 70.29589778076665,
                "f1": 67.7577001808977,
                "main_score": 70.29589778076665
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 66.31136516476126,
                "f1": 64.52032955983242,
                "main_score": 66.31136516476126
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 65.54472091459314,
                "f1": 61.47903120066317,
                "main_score": 65.54472091459314
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 61.45595158036314,
                "f1": 58.0891846024637,
                "main_score": 61.45595158036314
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 65.47074646940149,
                "f1": 62.84830858877575,
                "main_score": 65.47074646940149
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 58.046402151983855,
                "f1": 55.269074430533195,
                "main_score": 58.046402151983855
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 64.06523201075991,
                "f1": 61.35339643021369,
                "main_score": 64.06523201075991
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 60.954942837928726,
                "f1": 57.07035922704846,
                "main_score": 60.954942837928726
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 57.404169468728995,
                "f1": 53.94259011839138,
                "main_score": 57.404169468728995
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 64.16610625420309,
                "f1": 61.337103431499365,
                "main_score": 64.16610625420309
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 52.262945527908535,
                "f1": 49.7610691598921,
                "main_score": 52.262945527908535
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 65.54472091459314,
                "f1": 63.469099018440154,
                "main_score": 65.54472091459314
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 68.22797579018157,
                "f1": 64.89098471083001,
                "main_score": 68.22797579018157
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 50.847343644922674,
                "f1": 47.8536963168393,
                "main_score": 50.847343644922674
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 48.45326160053799,
                "f1": 46.370078045805556,
                "main_score": 48.45326160053799
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 42.83120376597175,
                "f1": 39.68948521599982,
                "main_score": 42.83120376597175
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 57.5084061869536,
                "f1": 53.961876160401545,
                "main_score": 57.5084061869536
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 63.7895090786819,
                "f1": 61.134223684676,
                "main_score": 63.7895090786819
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 54.98991257565569,
                "f1": 52.579862862826296,
                "main_score": 54.98991257565569
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 61.90316072629456,
                "f1": 58.203024538290336,
                "main_score": 61.90316072629456
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 57.09818426361802,
                "f1": 54.22718458445455,
                "main_score": 57.09818426361802
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 58.991257565568255,
                "f1": 55.84892781767421,
                "main_score": 58.991257565568255
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 55.901143241425686,
                "f1": 52.25264332199797,
                "main_score": 55.901143241425686
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 61.96368527236047,
                "f1": 58.927243876153454,
                "main_score": 61.96368527236047
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 65.64223268325489,
                "f1": 62.340453718379706,
                "main_score": 65.64223268325489
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 64.52589105581708,
                "f1": 61.661113187022174,
                "main_score": 64.52589105581708
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 66.84599865501009,
                "f1": 64.59342572873005,
                "main_score": 66.84599865501009
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 60.81035642232684,
                "f1": 57.5169089806797,
                "main_score": 60.81035642232684
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 65.75991930060525,
                "f1": 62.89531115787938,
                "main_score": 65.75991930060525
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 56.51647612642906,
                "f1": 54.33154780100043,
                "main_score": 56.51647612642906
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 57.985877605917956,
                "f1": 54.46187524463802,
                "main_score": 57.985877605917956
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 65.03026227303296,
                "f1": 62.34377392877748,
                "main_score": 65.03026227303296
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 53.567585743106925,
                "f1": 50.73770655983206,
                "main_score": 53.567585743106925
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 57.2595830531271,
                "f1": 53.657327291708626,
                "main_score": 57.2595830531271
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 57.82784129119032,
                "f1": 54.82518072665301,
                "main_score": 57.82784129119032
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 64.06859448554137,
                "f1": 63.00185280500495,
                "main_score": 64.06859448554137
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 58.91055817081371,
                "f1": 55.54116301224262,
                "main_score": 58.91055817081371
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 63.54404841963686,
                "f1": 59.57650946030184,
                "main_score": 63.54404841963686
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 59.27706792199059,
                "f1": 56.50010066083435,
                "main_score": 59.27706792199059
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 64.0719569603228,
                "f1": 61.817075925647956,
                "main_score": 64.0719569603228
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 68.23806321452591,
                "f1": 65.24917026029749,
                "main_score": 68.23806321452591
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 62.53530598520511,
                "f1": 61.71131132295768,
                "main_score": 62.53530598520511
            }
        ]
    }
}