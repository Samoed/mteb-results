{
    "dataset_revision": "e4e08e0b7dbe3c8700f0daef558ff32256715259",
    "task_name": "QuoraRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "map_at_1": 64.706,
                "map_at_10": 77.777,
                "map_at_100": 78.509,
                "map_at_1000": 78.537,
                "map_at_20": 78.237,
                "map_at_3": 74.802,
                "map_at_5": 76.655,
                "mrr_at_1": 74.62,
                "mrr_at_10": 81.817,
                "mrr_at_100": 82.021,
                "mrr_at_1000": 82.025,
                "mrr_at_20": 81.962,
                "mrr_at_3": 80.452,
                "mrr_at_5": 81.352,
                "ndcg_at_1": 74.64,
                "ndcg_at_10": 82.30499999999999,
                "ndcg_at_100": 84.21,
                "ndcg_at_1000": 84.505,
                "ndcg_at_20": 83.255,
                "ndcg_at_3": 78.851,
                "ndcg_at_5": 80.72200000000001,
                "precision_at_1": 74.64,
                "precision_at_10": 12.457,
                "precision_at_100": 1.473,
                "precision_at_1000": 0.155,
                "precision_at_20": 6.677,
                "precision_at_3": 34.29,
                "precision_at_5": 22.7,
                "recall_at_1": 64.706,
                "recall_at_10": 91.01,
                "recall_at_100": 98.039,
                "recall_at_1000": 99.66000000000001,
                "recall_at_20": 94.184,
                "recall_at_3": 81.12700000000001,
                "recall_at_5": 86.319,
                "main_score": 82.30499999999999
            }
        ]
    }
}