{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 47.79757901815736,
                "f1": 45.078250421193324,
                "main_score": 47.79757901815736
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 7.078009414929388,
                "f1": 4.0122456300041645,
                "main_score": 7.078009414929388
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 22.831203765971754,
                "f1": 20.131610050816555,
                "main_score": 22.831203765971754
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 44.952925353059854,
                "f1": 42.6865575762921,
                "main_score": 44.952925353059854
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 16.593813046402154,
                "f1": 14.087144503044291,
                "main_score": 16.593813046402154
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 37.91862811028917,
                "f1": 34.968402727911915,
                "main_score": 37.91862811028917
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 51.923335574983184,
                "f1": 49.357147840776335,
                "main_score": 51.923335574983184
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 58.73570948217889,
                "f1": 54.92084137819753,
                "main_score": 58.73570948217889
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 42.995965030262276,
                "f1": 38.47512542753069,
                "main_score": 42.995965030262276
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 77.42098184263618,
                "f1": 77.03413816048877,
                "main_score": 77.42098184263618
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 54.46536650975118,
                "f1": 53.08520810835907,
                "main_score": 54.46536650975118
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 30.578345662407525,
                "f1": 28.822998245702635,
                "main_score": 30.578345662407525
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 43.567585743106925,
                "f1": 39.79216651714347,
                "main_score": 43.567585743106925
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 56.98722259583053,
                "f1": 55.31168113501439,
                "main_score": 56.98722259583053
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 28.076664425016812,
                "f1": 24.927348965627573,
                "main_score": 28.076664425016812
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 18.096839273705445,
                "f1": 17.386603595777103,
                "main_score": 18.096839273705445
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 41.73839946200403,
                "f1": 38.65545902563735,
                "main_score": 41.73839946200403
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 11.536650975117688,
                "f1": 10.898336694524854,
                "main_score": 11.536650975117688
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 46.9502353732347,
                "f1": 44.332561323528644,
                "main_score": 46.9502353732347
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 42.777404169468724,
                "f1": 39.378117766055354,
                "main_score": 42.777404169468724
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 54.6469401479489,
                "f1": 52.512025274851794,
                "main_score": 54.6469401479489
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 35.90114324142569,
                "f1": 34.90331274712605,
                "main_score": 35.90114324142569
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 42.51176866173504,
                "f1": 39.417541845685676,
                "main_score": 42.51176866173504
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 13.799596503026226,
                "f1": 11.587556164962251,
                "main_score": 13.799596503026226
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 9.44855413584398,
                "f1": 4.30711077076907,
                "main_score": 9.44855413584398
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 8.157363819771351,
                "f1": 5.5588908736809515,
                "main_score": 8.157363819771351
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 19.909213180901144,
                "f1": 18.964761241087984,
                "main_score": 19.909213180901144
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 40.47747141896436,
                "f1": 38.17159556642586,
                "main_score": 40.47747141896436
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 6.701412239408204,
                "f1": 3.621974155647488,
                "main_score": 6.701412239408204
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 28.55413584398117,
                "f1": 26.582548923662753,
                "main_score": 28.55413584398117
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 46.617350369872234,
                "f1": 41.35397419267425,
                "main_score": 46.617350369872234
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 9.976462676529927,
                "f1": 5.900764382768462,
                "main_score": 9.976462676529927
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 50.894418291862806,
                "f1": 47.70929403771086,
                "main_score": 50.894418291862806
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 51.761936785474106,
                "f1": 48.42797973062516,
                "main_score": 51.761936785474106
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 46.21385339609952,
                "f1": 43.7081546200347,
                "main_score": 46.21385339609952
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 55.59852051109617,
                "f1": 54.19610878409633,
                "main_score": 55.59852051109617
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 50.54135843981169,
                "f1": 47.79393938467311,
                "main_score": 50.54135843981169
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 37.73032952252858,
                "f1": 35.96450149708041,
                "main_score": 37.73032952252858
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 41.67114996637525,
                "f1": 40.28283538885605,
                "main_score": 41.67114996637525
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 47.38063214525891,
                "f1": 44.93264016007152,
                "main_score": 47.38063214525891
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 49.28379287155347,
                "f1": 46.25486396570196,
                "main_score": 49.28379287155347
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 44.18291862811029,
                "f1": 41.17519157172804,
                "main_score": 44.18291862811029
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 12.599193006052452,
                "f1": 11.129236666238377,
                "main_score": 12.599193006052452
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 7.017484868863484,
                "f1": 3.9665415549749077,
                "main_score": 7.017484868863484
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 19.788164088769335,
                "f1": 15.783384761347582,
                "main_score": 19.788164088769335
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 50.35978480161398,
                "f1": 47.30586047800275,
                "main_score": 50.35978480161398
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 45.484196368527236,
                "f1": 44.65101184252231,
                "main_score": 45.484196368527236
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 23.681909885675857,
                "f1": 22.247817138937524,
                "main_score": 23.681909885675857
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 41.63080026899798,
                "f1": 39.546896741744,
                "main_score": 41.63080026899798
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 30.141223940820446,
                "f1": 28.177838960078123,
                "main_score": 30.141223940820446
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 27.515131136516473,
                "f1": 26.514325837594654,
                "main_score": 27.515131136516473
            }
        ]
    }
}