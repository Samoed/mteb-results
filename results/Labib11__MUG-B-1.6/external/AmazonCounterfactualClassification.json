{
    "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
    "task_name": "AmazonCounterfactualClassification",
    "evaluation_time": NaN,
    "mteb_version": "unknown",
    "scores": {
        "test": [
            {
                "hf_subset": "en-ext",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 74.04047976011994,
                "ap": 23.622442298323236,
                "f1": 61.681362134359354,
                "main_score": 74.04047976011994
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 72.38805970149255,
                "ap": 35.14527522183942,
                "f1": 66.40004634079556,
                "main_score": 72.38805970149255
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 54.3254817987152,
                "ap": 71.95259605308317,
                "f1": 52.50731386267296,
                "main_score": 54.3254817987152
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 56.33832976445397,
                "ap": 12.671021199223937,
                "f1": 46.127586182990605,
                "main_score": 56.33832976445397
            }
        ]
    }
}