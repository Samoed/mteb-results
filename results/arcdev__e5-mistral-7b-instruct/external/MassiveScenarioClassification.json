{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 73.37256220578345,
                "f1": 72.56456170538992,
                "main_score": 73.37256220578345
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 47.205783456624076,
                "f1": 45.905999859074434,
                "main_score": 47.205783456624076
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 69.8352387357095,
                "f1": 69.43553987525273,
                "main_score": 69.8352387357095
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 67.00403496973773,
                "f1": 65.97477215779143,
                "main_score": 67.00403496973773
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 68.04976462676531,
                "f1": 67.24581993778398,
                "main_score": 68.04976462676531
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 61.882985877605925,
                "f1": 59.995293199988794,
                "main_score": 61.882985877605925
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 76.75857431069267,
                "f1": 76.52031675299841,
                "main_score": 76.75857431069267
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 79.03496973772697,
                "f1": 79.25548063175344,
                "main_score": 79.03496973772697
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 72.96570275722931,
                "f1": 72.19110435289122,
                "main_score": 72.96570275722931
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 82.38735709482178,
                "f1": 82.34495627619785,
                "main_score": 82.38735709482178
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 78.83994620040352,
                "f1": 78.91526355393667,
                "main_score": 78.83994620040352
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 76.7350369872226,
                "f1": 75.919437344927,
                "main_score": 76.7350369872226
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 71.21721587088096,
                "f1": 70.82973286243262,
                "main_score": 71.21721587088096
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 78.59784801613988,
                "f1": 78.47383161087423,
                "main_score": 78.59784801613988
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 69.64021519838602,
                "f1": 68.45118053027653,
                "main_score": 69.64021519838602
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 73.51042367182245,
                "f1": 72.90013022879003,
                "main_score": 73.51042367182245
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 74.0551445864156,
                "f1": 73.45871761713292,
                "main_score": 74.0551445864156
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 59.54606590450571,
                "f1": 57.72711794953869,
                "main_score": 59.54606590450571
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 77.40753194351042,
                "f1": 76.8157455506521,
                "main_score": 77.40753194351042
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 66.58372562205783,
                "f1": 65.2654868709758,
                "main_score": 66.58372562205783
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 78.39273705447208,
                "f1": 78.3592956594837,
                "main_score": 78.39273705447208
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 79.62004034969739,
                "f1": 79.78673754501855,
                "main_score": 79.62004034969739
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 64.29051782111634,
                "f1": 63.12502587609454,
                "main_score": 64.29051782111634
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 57.51849361129791,
                "f1": 56.32320906403241,
                "main_score": 57.51849361129791
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 52.41761936785474,
                "f1": 49.113762010098306,
                "main_score": 52.41761936785474
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 58.547410894418284,
                "f1": 56.87580674198118,
                "main_score": 58.547410894418284
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 78.89038332212507,
                "f1": 79.09210140529848,
                "main_score": 78.89038332212507
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 63.503698722259585,
                "f1": 61.45718858568352,
                "main_score": 63.503698722259585
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 54.02824478816408,
                "f1": 52.732738981386504,
                "main_score": 54.02824478816408
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 54.23671822461331,
                "f1": 52.688080372545286,
                "main_score": 54.23671822461331
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 75.5312710154674,
                "f1": 74.59368478550698,
                "main_score": 75.5312710154674
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 52.192333557498316,
                "f1": 50.18302290152229,
                "main_score": 52.192333557498316
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 75.6960322797579,
                "f1": 75.25331182714856,
                "main_score": 75.6960322797579
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 78.47679892400808,
                "f1": 78.24044732352424,
                "main_score": 78.47679892400808
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 77.36718224613315,
                "f1": 77.2714452985389,
                "main_score": 77.36718224613315
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 77.96234028244788,
                "f1": 78.21282127011372,
                "main_score": 77.96234028244788
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 73.19435104236717,
                "f1": 73.1963711292812,
                "main_score": 73.19435104236717
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 80.52118359112306,
                "f1": 80.4179964390288,
                "main_score": 80.52118359112306
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 73.65837256220577,
                "f1": 73.07156989634905,
                "main_score": 73.65837256220577
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 64.02824478816409,
                "f1": 62.972399027713664,
                "main_score": 64.02824478816409
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 78.87020847343645,
                "f1": 78.224240866849,
                "main_score": 78.87020847343645
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 64.6570275722932,
                "f1": 63.274871811412545,
                "main_score": 64.6570275722932
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 57.760591795561524,
                "f1": 56.73711528075771,
                "main_score": 57.760591795561524
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 57.26967047747142,
                "f1": 55.74735330863165,
                "main_score": 57.26967047747142
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 72.46133154001345,
                "f1": 71.9644168952811,
                "main_score": 72.46133154001345
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 73.70880968392737,
                "f1": 73.61543141070884,
                "main_score": 73.70880968392737
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 75.0437121721587,
                "f1": 74.83359868879921,
                "main_score": 75.0437121721587
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 67.05110961667788,
                "f1": 66.25869819274315,
                "main_score": 67.05110961667788
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 75.52118359112306,
                "f1": 75.92098546052303,
                "main_score": 75.52118359112306
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 79.92938802958977,
                "f1": 79.79833572573796,
                "main_score": 79.92938802958977
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 76.86617350369872,
                "f1": 77.42645654909516,
                "main_score": 76.86617350369872
            }
        ]
    }
}