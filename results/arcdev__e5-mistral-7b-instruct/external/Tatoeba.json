{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 70.5,
                "f1": 67.05501804646966,
                "precision": 65.73261904761904,
                "recall": 70.5,
                "main_score": 67.05501804646966
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 75.14450867052022,
                "f1": 70.98265895953759,
                "precision": 69.26782273603082,
                "recall": 75.14450867052022,
                "main_score": 70.98265895953759
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 33.170731707317074,
                "f1": 29.92876500193573,
                "precision": 28.669145894755648,
                "recall": 33.170731707317074,
                "main_score": 29.92876500193573
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.5,
                "f1": 94.13333333333333,
                "precision": 93.46666666666667,
                "recall": 95.5,
                "main_score": 94.13333333333333
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 99.6,
                "f1": 99.46666666666665,
                "precision": 99.4,
                "recall": 99.6,
                "main_score": 99.46666666666665
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.2,
                "f1": 96.39999999999999,
                "precision": 96.0,
                "recall": 97.2,
                "main_score": 96.39999999999999
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.5,
                "f1": 92.99666666666667,
                "precision": 92.31666666666666,
                "recall": 94.5,
                "main_score": 92.99666666666667
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 85.82089552238806,
                "f1": 81.59203980099502,
                "precision": 79.60199004975124,
                "recall": 85.82089552238806,
                "main_score": 81.59203980099502
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 79.5,
                "f1": 75.11246031746032,
                "precision": 73.38734126984127,
                "recall": 79.5,
                "main_score": 75.11246031746032
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 44.390243902439025,
                "f1": 38.48896631823461,
                "precision": 36.57220286488579,
                "recall": 44.390243902439025,
                "main_score": 38.48896631823461
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.2,
                "f1": 87.57333333333334,
                "precision": 86.34166666666665,
                "recall": 90.2,
                "main_score": 87.57333333333334
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 88.82138517618469,
                "f1": 85.98651854423423,
                "precision": 84.79257073424753,
                "recall": 88.82138517618469,
                "main_score": 85.98651854423423
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 77.04347826086956,
                "f1": 72.32108147606868,
                "precision": 70.37207357859532,
                "recall": 77.04347826086956,
                "main_score": 72.32108147606868
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 53.04347826086957,
                "f1": 46.88868184955141,
                "precision": 44.71730105643149,
                "recall": 53.04347826086957,
                "main_score": 46.88868184955141
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 68.0,
                "f1": 62.891813186813195,
                "precision": 61.037906162464985,
                "recall": 68.0,
                "main_score": 62.891813186813195
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 86.3,
                "f1": 82.82000000000001,
                "precision": 81.25690476190475,
                "recall": 86.3,
                "main_score": 82.82000000000001
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 68.87816646562122,
                "f1": 63.53054933272062,
                "precision": 61.47807816331196,
                "recall": 68.87816646562122,
                "main_score": 63.53054933272062
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 74.4,
                "f1": 68.99388888888889,
                "precision": 66.81035714285713,
                "recall": 74.4,
                "main_score": 68.99388888888889
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.5,
                "f1": 87.93666666666667,
                "precision": 86.825,
                "recall": 90.5,
                "main_score": 87.93666666666667
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 90.7,
                "f1": 88.09,
                "precision": 86.85833333333333,
                "recall": 90.7,
                "main_score": 88.09
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 67.61904761904762,
                "f1": 62.30239247214037,
                "precision": 60.340702947845806,
                "recall": 67.61904761904762,
                "main_score": 62.30239247214037
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 77.9,
                "f1": 73.81285714285714,
                "precision": 72.21570818070818,
                "recall": 77.9,
                "main_score": 73.81285714285714
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 91.8,
                "f1": 89.66666666666667,
                "precision": 88.66666666666666,
                "recall": 91.8,
                "main_score": 89.66666666666667
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.6,
                "f1": 96.85666666666665,
                "precision": 96.50833333333333,
                "recall": 97.6,
                "main_score": 96.85666666666665
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.39999999999999,
                "f1": 93.98333333333333,
                "precision": 93.30000000000001,
                "recall": 95.39999999999999,
                "main_score": 93.98333333333333
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 85.0,
                "f1": 81.31538461538462,
                "precision": 79.70666666666666,
                "recall": 85.0,
                "main_score": 81.31538461538462
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 91.60000000000001,
                "f1": 89.81888888888888,
                "precision": 89.08583333333333,
                "recall": 91.60000000000001,
                "main_score": 89.81888888888888
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 44.3,
                "f1": 38.8623088023088,
                "precision": 37.03755623461505,
                "recall": 44.3,
                "main_score": 38.8623088023088
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 93.75,
                "precision": 93.05,
                "recall": 95.19999999999999,
                "main_score": 93.75
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 99.1,
                "f1": 98.8,
                "precision": 98.65,
                "recall": 99.1,
                "main_score": 98.8
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 69.6765498652291,
                "f1": 63.991785393402644,
                "precision": 61.7343729944808,
                "recall": 69.6765498652291,
                "main_score": 63.991785393402644
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 50.0,
                "f1": 42.79341029341029,
                "precision": 40.25098358431692,
                "recall": 50.0,
                "main_score": 42.79341029341029
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 89.7,
                "f1": 87.19023809523809,
                "precision": 86.12595238095237,
                "recall": 89.7,
                "main_score": 87.19023809523809
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 42.72727272727273,
                "f1": 37.78789518562245,
                "precision": 36.24208471267295,
                "recall": 42.72727272727273,
                "main_score": 37.78789518562245
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 75.26205450733752,
                "f1": 70.72842833849123,
                "precision": 68.93256464011182,
                "recall": 75.26205450733752,
                "main_score": 70.72842833849123
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 93.96666666666668,
                "precision": 93.42,
                "recall": 95.19999999999999,
                "main_score": 93.96666666666668
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 76.26459143968872,
                "f1": 72.40190419178747,
                "precision": 70.84954604409856,
                "recall": 76.26459143968872,
                "main_score": 72.40190419178747
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 59.82905982905983,
                "f1": 52.2100122100122,
                "precision": 49.52516619183286,
                "recall": 59.82905982905983,
                "main_score": 52.2100122100122
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 81.69999999999999,
                "f1": 77.41714285714286,
                "precision": 75.64833333333334,
                "recall": 81.69999999999999,
                "main_score": 77.41714285714286
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.5,
                "f1": 94.45,
                "precision": 93.93333333333334,
                "recall": 95.5,
                "main_score": 94.45
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 58.41121495327103,
                "f1": 52.73495974430554,
                "precision": 50.717067200712066,
                "recall": 58.41121495327103,
                "main_score": 52.73495974430554
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 73.3,
                "f1": 69.20371794871795,
                "precision": 67.6597557997558,
                "recall": 73.3,
                "main_score": 69.20371794871795
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.5,
                "f1": 95.51666666666667,
                "precision": 95.05,
                "recall": 96.5,
                "main_score": 95.51666666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 78.4,
                "f1": 73.88856643356644,
                "precision": 72.01373015873016,
                "recall": 78.4,
                "main_score": 73.88856643356644
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.3,
                "f1": 94.09666666666668,
                "precision": 93.53333333333332,
                "recall": 95.3,
                "main_score": 94.09666666666668
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.7,
                "f1": 91.94,
                "precision": 91.10833333333333,
                "recall": 93.7,
                "main_score": 91.94
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 96.8,
                "f1": 95.89999999999999,
                "precision": 95.46666666666668,
                "recall": 96.8,
                "main_score": 95.89999999999999
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 70.5,
                "f1": 66.00635642135641,
                "precision": 64.36345238095238,
                "recall": 70.5,
                "main_score": 66.00635642135641
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.4,
                "f1": 90.44388888888889,
                "precision": 89.5767857142857,
                "recall": 92.4,
                "main_score": 90.44388888888889
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 48.0,
                "f1": 43.15372775372776,
                "precision": 41.53152510162313,
                "recall": 48.0,
                "main_score": 43.15372775372776
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 16.7,
                "f1": 14.198431372549017,
                "precision": 13.411765873015872,
                "recall": 16.7,
                "main_score": 14.198431372549017
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 85.7,
                "f1": 81.81666666666666,
                "precision": 80.10833333333332,
                "recall": 85.7,
                "main_score": 81.81666666666666
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 69.64285714285714,
                "f1": 64.745670995671,
                "precision": 62.916666666666664,
                "recall": 69.64285714285714,
                "main_score": 64.745670995671
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 54.665203073545555,
                "f1": 48.55366630916923,
                "precision": 46.35683318998357,
                "recall": 54.665203073545555,
                "main_score": 48.55366630916923
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 4.8,
                "f1": 3.808587223587223,
                "precision": 3.5653174603174604,
                "recall": 4.8,
                "main_score": 3.808587223587223
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.6,
                "f1": 95.77333333333333,
                "precision": 95.39166666666667,
                "recall": 96.6,
                "main_score": 95.77333333333333
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.39999999999999,
                "f1": 94.44,
                "precision": 93.975,
                "recall": 95.39999999999999,
                "main_score": 94.44
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 42.0,
                "f1": 37.024908424908425,
                "precision": 35.365992063492065,
                "recall": 42.0,
                "main_score": 37.024908424908425
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 66.7,
                "f1": 62.20460835058661,
                "precision": 60.590134587634594,
                "recall": 66.7,
                "main_score": 62.20460835058661
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.3,
                "f1": 96.46666666666667,
                "precision": 96.06666666666668,
                "recall": 97.3,
                "main_score": 96.46666666666667
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 47.3,
                "f1": 41.96905408317173,
                "precision": 40.18741402116402,
                "recall": 47.3,
                "main_score": 41.96905408317173
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 80.2,
                "f1": 76.22690476190476,
                "precision": 74.63539682539682,
                "recall": 80.2,
                "main_score": 76.22690476190476
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.0,
                "f1": 94.83333333333333,
                "precision": 94.26666666666668,
                "recall": 96.0,
                "main_score": 94.83333333333333
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 89.7,
                "f1": 87.24333333333334,
                "precision": 86.17,
                "recall": 89.7,
                "main_score": 87.24333333333334
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 50.36496350364964,
                "f1": 44.795520780922246,
                "precision": 43.09002433090024,
                "recall": 50.36496350364964,
                "main_score": 44.795520780922246
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 18.8,
                "f1": 16.242864357864356,
                "precision": 15.466596638655464,
                "recall": 18.8,
                "main_score": 16.242864357864356
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 93.92333333333333,
                "precision": 93.30833333333332,
                "recall": 95.19999999999999,
                "main_score": 93.92333333333333
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 93.4,
                "f1": 91.42333333333333,
                "precision": 90.50833333333334,
                "recall": 93.4,
                "main_score": 91.42333333333333
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 26.190476190476193,
                "f1": 22.05208151636723,
                "precision": 21.09292328042328,
                "recall": 26.190476190476193,
                "main_score": 22.05208151636723
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 17.2,
                "f1": 14.021009731460952,
                "precision": 13.1389886698243,
                "recall": 17.2,
                "main_score": 14.021009731460952
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 78.67494824016563,
                "f1": 74.24430641821947,
                "precision": 72.50747642051991,
                "recall": 78.67494824016563,
                "main_score": 74.24430641821947
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 94.19999999999999,
                "f1": 92.54,
                "precision": 91.75833333333334,
                "recall": 94.19999999999999,
                "main_score": 92.54
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.2,
                "f1": 87.78666666666666,
                "precision": 86.69833333333334,
                "recall": 90.2,
                "main_score": 87.78666666666666
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 14.7,
                "f1": 12.19206214842218,
                "precision": 11.526261904761904,
                "recall": 14.7,
                "main_score": 12.19206214842218
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 73.16017316017316,
                "f1": 67.44858316286889,
                "precision": 65.23809523809523,
                "recall": 73.16017316017316,
                "main_score": 67.44858316286889
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 75.19083969465649,
                "f1": 70.33078880407125,
                "precision": 68.3969465648855,
                "recall": 75.19083969465649,
                "main_score": 70.33078880407125
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 62.154294032023294,
                "f1": 55.86030821838681,
                "precision": 53.53509623160277,
                "recall": 62.154294032023294,
                "main_score": 55.86030821838681
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 86.8,
                "f1": 83.9652380952381,
                "precision": 82.84242424242424,
                "recall": 86.8,
                "main_score": 83.9652380952381
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.50282485875707,
                "f1": 91.54425612052731,
                "precision": 90.65442561205272,
                "recall": 93.50282485875707,
                "main_score": 91.54425612052731
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.4,
                "f1": 9.189775870222714,
                "precision": 8.66189886502811,
                "recall": 11.4,
                "main_score": 9.189775870222714
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.4,
                "f1": 91.88666666666666,
                "precision": 91.21444444444444,
                "recall": 93.4,
                "main_score": 91.88666666666666
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 46.0,
                "f1": 40.51069226095542,
                "precision": 38.57804926010808,
                "recall": 46.0,
                "main_score": 40.51069226095542
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 91.0,
                "f1": 89.11333333333333,
                "precision": 88.27000000000001,
                "recall": 91.0,
                "main_score": 89.11333333333333
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.39999999999999,
                "f1": 92.95,
                "precision": 92.27000000000001,
                "recall": 94.39999999999999,
                "main_score": 92.95
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 14.2,
                "f1": 11.73701698770113,
                "precision": 11.079207014736676,
                "recall": 14.2,
                "main_score": 11.73701698770113
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 65.14745308310992,
                "f1": 59.665707393589415,
                "precision": 57.560853653346946,
                "recall": 65.14745308310992,
                "main_score": 59.665707393589415
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 95.39999999999999,
                "f1": 94.0,
                "precision": 93.33333333333333,
                "recall": 95.39999999999999,
                "main_score": 94.0
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 69.56521739130434,
                "f1": 62.92490118577074,
                "precision": 60.27009222661397,
                "recall": 69.56521739130434,
                "main_score": 62.92490118577074
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 40.140845070422536,
                "f1": 35.96411804158283,
                "precision": 34.89075869357559,
                "recall": 40.140845070422536,
                "main_score": 35.96411804158283
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 65.86826347305389,
                "f1": 59.646248628284546,
                "precision": 57.22982606216139,
                "recall": 65.86826347305389,
                "main_score": 59.646248628284546
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.89999999999999,
                "f1": 93.48333333333333,
                "precision": 92.83666666666667,
                "recall": 94.89999999999999,
                "main_score": 93.48333333333333
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 47.783251231527096,
                "f1": 42.006447302013804,
                "precision": 40.12747105111637,
                "recall": 47.783251231527096,
                "main_score": 42.006447302013804
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 69.71830985915493,
                "f1": 64.80266212660578,
                "precision": 63.08098591549296,
                "recall": 69.71830985915493,
                "main_score": 64.80266212660578
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 67.94871794871796,
                "f1": 61.59912309912309,
                "precision": 59.17338217338218,
                "recall": 67.94871794871796,
                "main_score": 61.59912309912309
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 96.39999999999999,
                "f1": 95.28333333333335,
                "precision": 94.75,
                "recall": 96.39999999999999,
                "main_score": 95.28333333333335
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 70.14613778705638,
                "f1": 65.4349338900487,
                "precision": 63.57599255302805,
                "recall": 70.14613778705638,
                "main_score": 65.4349338900487
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 9.2,
                "f1": 7.622184434339607,
                "precision": 7.287048159682417,
                "recall": 9.2,
                "main_score": 7.622184434339607
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 77.85016286644951,
                "f1": 72.83387622149837,
                "precision": 70.58450959102424,
                "recall": 77.85016286644951,
                "main_score": 72.83387622149837
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.8,
                "f1": 88.84333333333333,
                "precision": 87.96666666666665,
                "recall": 90.8,
                "main_score": 88.84333333333333
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.6,
                "f1": 93.14,
                "precision": 92.49833333333333,
                "recall": 94.6,
                "main_score": 93.14
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 84.25196850393701,
                "f1": 80.94488188976378,
                "precision": 79.65879265091863,
                "recall": 84.25196850393701,
                "main_score": 80.94488188976378
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 89.5,
                "f1": 86.89666666666666,
                "precision": 85.7,
                "recall": 89.5,
                "main_score": 86.89666666666666
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 42.797783933518005,
                "f1": 37.30617360155193,
                "precision": 35.34933825792552,
                "recall": 42.797783933518005,
                "main_score": 37.30617360155193
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.1,
                "f1": 94.93333333333332,
                "precision": 94.38333333333333,
                "recall": 96.1,
                "main_score": 94.93333333333332
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 54.807692307692314,
                "f1": 49.506903353057204,
                "precision": 47.54807692307693,
                "recall": 54.807692307692314,
                "main_score": 49.506903353057204
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 87.1,
                "f1": 83.61857142857143,
                "precision": 81.975,
                "recall": 87.1,
                "main_score": 83.61857142857143
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 91.10000000000001,
                "f1": 88.76333333333332,
                "precision": 87.67,
                "recall": 91.10000000000001,
                "main_score": 88.76333333333332
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 93.10000000000001,
                "f1": 91.28999999999999,
                "precision": 90.44500000000001,
                "recall": 93.10000000000001,
                "main_score": 91.28999999999999
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 39.97641509433962,
                "f1": 33.12271889998028,
                "precision": 30.95185381542554,
                "recall": 39.97641509433962,
                "main_score": 33.12271889998028
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.60000000000001,
                "f1": 90.69,
                "precision": 89.84500000000001,
                "recall": 92.60000000000001,
                "main_score": 90.69
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 95.07299270072993,
                "f1": 93.64355231143554,
                "precision": 92.94403892944038,
                "recall": 95.07299270072993,
                "main_score": 93.64355231143554
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 91.9,
                "f1": 89.61333333333333,
                "precision": 88.53333333333333,
                "recall": 91.9,
                "main_score": 89.61333333333333
            }
        ]
    }
}