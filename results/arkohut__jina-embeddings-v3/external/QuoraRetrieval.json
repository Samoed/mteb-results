{
    "dataset_revision": "e4e08e0b7dbe3c8700f0daef558ff32256715259",
    "task_name": "QuoraRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "map_at_1": 71.419,
                "map_at_10": 85.542,
                "map_at_100": 86.161,
                "map_at_1000": 86.175,
                "map_at_20": 85.949,
                "map_at_3": 82.623,
                "map_at_5": 84.5,
                "mrr_at_1": 82.27,
                "mrr_at_10": 88.21900000000001,
                "mrr_at_100": 88.313,
                "mrr_at_1000": 88.31400000000001,
                "mrr_at_20": 88.286,
                "mrr_at_3": 87.325,
                "mrr_at_5": 87.97500000000001,
                "ndcg_at_1": 82.3,
                "ndcg_at_10": 89.088,
                "ndcg_at_100": 90.217,
                "ndcg_at_1000": 90.29700000000001,
                "ndcg_at_20": 89.697,
                "ndcg_at_3": 86.435,
                "ndcg_at_5": 87.966,
                "precision_at_1": 82.3,
                "precision_at_10": 13.527000000000001,
                "precision_at_100": 1.537,
                "precision_at_1000": 0.157,
                "precision_at_20": 7.165000000000001,
                "precision_at_3": 37.92,
                "precision_at_5": 24.914,
                "recall_at_1": 71.419,
                "recall_at_10": 95.831,
                "recall_at_100": 99.64,
                "recall_at_1000": 99.988,
                "recall_at_20": 97.76599999999999,
                "recall_at_3": 88.081,
                "recall_at_5": 92.50500000000001,
                "main_score": 89.088
            }
        ]
    }
}