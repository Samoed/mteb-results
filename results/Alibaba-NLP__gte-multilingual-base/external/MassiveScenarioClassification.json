{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 64.89912575655683,
                "main_score": 64.89912575655683
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 57.27975790181573,
                "main_score": 57.27975790181573
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 62.269670477471415,
                "main_score": 62.269670477471415
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 65.10423671822461,
                "main_score": 65.10423671822461
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 62.40753194351043,
                "main_score": 62.40753194351043
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 55.369872225958304,
                "main_score": 55.369872225958304
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 71.60726294552792,
                "main_score": 71.60726294552792
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 70.30262273032952,
                "main_score": 70.30262273032952
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 69.52925353059851,
                "main_score": 69.52925353059851
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 76.28446536650976,
                "main_score": 76.28446536650976
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 72.45460659045058,
                "main_score": 72.45460659045058
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 70.26563550773368,
                "main_score": 70.26563550773368
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 67.20578345662408,
                "main_score": 67.20578345662408
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 72.64963012777405,
                "main_score": 72.64963012777405
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 61.698049764626774,
                "main_score": 61.698049764626774
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 70.14458641560188,
                "main_score": 70.14458641560188
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 70.51445864156018,
                "main_score": 70.51445864156018
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 60.13786146603901,
                "main_score": 60.13786146603901
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 70.61533288500337,
                "main_score": 70.61533288500337
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 61.526563550773375,
                "main_score": 61.526563550773375
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 71.99731002017484,
                "main_score": 71.99731002017484
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 71.59381304640216,
                "main_score": 71.59381304640216
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 57.010759919300604,
                "main_score": 57.010759919300604
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 53.26160053799597,
                "main_score": 53.26160053799597
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 57.800941492938804,
                "main_score": 57.800941492938804
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 62.387357094821795,
                "main_score": 62.387357094821795
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 69.5359784801614,
                "main_score": 69.5359784801614
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 63.36919973100203,
                "main_score": 63.36919973100203
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 64.81506388702084,
                "main_score": 64.81506388702084
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 59.35104236718225,
                "main_score": 59.35104236718225
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 66.67787491593813,
                "main_score": 66.67787491593813
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 59.4250168123739,
                "main_score": 59.4250168123739
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 71.49630127774043,
                "main_score": 71.49630127774043
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 71.95696032279758,
                "main_score": 71.95696032279758
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 70.11768661735036,
                "main_score": 70.11768661735036
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 71.86953597848016,
                "main_score": 71.86953597848016
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 68.51042367182247,
                "main_score": 68.51042367182247
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 71.65097511768661,
                "main_score": 71.65097511768661
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 66.81573638197713,
                "main_score": 66.81573638197713
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 65.26227303295225,
                "main_score": 65.26227303295225
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 72.51513113651646,
                "main_score": 72.51513113651646
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 58.29858776059179,
                "main_score": 58.29858776059179
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 62.72696704774714,
                "main_score": 62.72696704774714
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 66.57700067249496,
                "main_score": 66.57700067249496
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 68.22797579018157,
                "main_score": 68.22797579018157
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 61.97041022192333,
                "main_score": 61.97041022192333
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 70.72629455279085,
                "main_score": 70.72629455279085
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 63.16072629455278,
                "main_score": 63.16072629455278
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 67.92199058507062,
                "main_score": 67.92199058507062
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 74.40484196368527,
                "main_score": 74.40484196368527
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 71.61398789509079,
                "main_score": 71.61398789509079
            }
        ]
    }
}