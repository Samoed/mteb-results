{
    "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
    "task_name": "NTREXBitextMining",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "arb_Arab-hun_Latn",
                "languages": [
                    "arb-Arab",
                    "hun-Latn"
                ],
                "accuracy": 85.678517776665,
                "f1": 81.92049979731502,
                "main_score": 81.92049979731502,
                "precision": 80.21115005842097,
                "recall": 85.678517776665
            },
            {
                "hf_subset": "ben_Beng-hun_Latn",
                "languages": [
                    "ben-Beng",
                    "hun-Latn"
                ],
                "accuracy": 44.566850275413124,
                "f1": 39.07033025889276,
                "main_score": 39.07033025889276,
                "precision": 37.07348327291399,
                "recall": 44.566850275413124
            },
            {
                "hf_subset": "deu_Latn-hun_Latn",
                "languages": [
                    "deu-Latn",
                    "hun-Latn"
                ],
                "accuracy": 93.44016024036054,
                "f1": 91.61909530963112,
                "main_score": 91.61909530963112,
                "precision": 90.75279586045735,
                "recall": 93.44016024036054
            },
            {
                "hf_subset": "ell_Grek-hun_Latn",
                "languages": [
                    "ell-Grek",
                    "hun-Latn"
                ],
                "accuracy": 91.4371557336004,
                "f1": 89.0261582850466,
                "main_score": 89.0261582850466,
                "precision": 87.9043565348022,
                "recall": 91.4371557336004
            },
            {
                "hf_subset": "eng_Latn-hun_Latn",
                "languages": [
                    "eng-Latn",
                    "hun-Latn"
                ],
                "accuracy": 94.44166249374061,
                "f1": 92.8092138207311,
                "main_score": 92.8092138207311,
                "precision": 92.0422300116842,
                "recall": 94.44166249374061
            },
            {
                "hf_subset": "fas_Arab-hun_Latn",
                "languages": [
                    "fas-Arab",
                    "hun-Latn"
                ],
                "accuracy": 89.53430145217827,
                "f1": 86.72270310227245,
                "main_score": 86.72270310227245,
                "precision": 85.42814221331997,
                "recall": 89.53430145217827
            },
            {
                "hf_subset": "fin_Latn-hun_Latn",
                "languages": [
                    "fin-Latn",
                    "hun-Latn"
                ],
                "accuracy": 90.98647971957938,
                "f1": 88.44600233683859,
                "main_score": 88.44600233683859,
                "precision": 87.2575529961609,
                "recall": 90.98647971957938
            },
            {
                "hf_subset": "fra_Latn-hun_Latn",
                "languages": [
                    "fra-Latn",
                    "hun-Latn"
                ],
                "accuracy": 92.28843264897347,
                "f1": 90.12518778167251,
                "main_score": 90.12518778167251,
                "precision": 89.12535469871473,
                "recall": 92.28843264897347
            },
            {
                "hf_subset": "heb_Hebr-hun_Latn",
                "languages": [
                    "heb-Hebr",
                    "hun-Latn"
                ],
                "accuracy": 87.33099649474211,
                "f1": 83.88582874311467,
                "main_score": 83.88582874311467,
                "precision": 82.31263562009681,
                "recall": 87.33099649474211
            },
            {
                "hf_subset": "hin_Deva-hun_Latn",
                "languages": [
                    "hin-Deva",
                    "hun-Latn"
                ],
                "accuracy": 86.52979469203805,
                "f1": 83.08240137984755,
                "main_score": 83.08240137984755,
                "precision": 81.51352028042064,
                "recall": 86.52979469203805
            },
            {
                "hf_subset": "hun_Latn-arb_Arab",
                "languages": [
                    "hun-Latn",
                    "arb-Arab"
                ],
                "accuracy": 86.73009514271406,
                "f1": 83.12397167179341,
                "main_score": 83.12397167179341,
                "precision": 81.47805040894676,
                "recall": 86.73009514271406
            },
            {
                "hf_subset": "hun_Latn-ben_Beng",
                "languages": [
                    "hun-Latn",
                    "ben-Beng"
                ],
                "accuracy": 41.16174261392088,
                "f1": 32.73025519520262,
                "main_score": 32.73025519520262,
                "precision": 29.859172986363774,
                "recall": 41.16174261392088
            },
            {
                "hf_subset": "hun_Latn-deu_Latn",
                "languages": [
                    "hun-Latn",
                    "deu-Latn"
                ],
                "accuracy": 93.39008512769153,
                "f1": 91.5456518110499,
                "main_score": 91.5456518110499,
                "precision": 90.66099148723085,
                "recall": 93.39008512769153
            },
            {
                "hf_subset": "hun_Latn-ell_Grek",
                "languages": [
                    "hun-Latn",
                    "ell-Grek"
                ],
                "accuracy": 92.03805708562844,
                "f1": 89.81305291270239,
                "main_score": 89.81305291270239,
                "precision": 88.78317476214322,
                "recall": 92.03805708562844
            },
            {
                "hf_subset": "hun_Latn-eng_Latn",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.74211316975463,
                "f1": 93.23985978968453,
                "main_score": 93.23985978968453,
                "precision": 92.51377065598398,
                "recall": 94.74211316975463
            },
            {
                "hf_subset": "hun_Latn-fas_Arab",
                "languages": [
                    "hun-Latn",
                    "fas-Arab"
                ],
                "accuracy": 88.5327991987982,
                "f1": 85.49240527457853,
                "main_score": 85.49240527457853,
                "precision": 84.10413238905979,
                "recall": 88.5327991987982
            },
            {
                "hf_subset": "hun_Latn-fin_Latn",
                "languages": [
                    "hun-Latn",
                    "fin-Latn"
                ],
                "accuracy": 90.23535302954431,
                "f1": 87.53296611584042,
                "main_score": 87.53296611584042,
                "precision": 86.26690035052579,
                "recall": 90.23535302954431
            },
            {
                "hf_subset": "hun_Latn-fra_Latn",
                "languages": [
                    "hun-Latn",
                    "fra-Latn"
                ],
                "accuracy": 92.63895843765648,
                "f1": 90.47070605908863,
                "main_score": 90.47070605908863,
                "precision": 89.42163244867301,
                "recall": 92.63895843765648
            },
            {
                "hf_subset": "hun_Latn-heb_Hebr",
                "languages": [
                    "hun-Latn",
                    "heb-Hebr"
                ],
                "accuracy": 86.62994491737606,
                "f1": 83.19388173168845,
                "main_score": 83.19388173168845,
                "precision": 81.65832081455517,
                "recall": 86.62994491737606
            },
            {
                "hf_subset": "hun_Latn-hin_Deva",
                "languages": [
                    "hun-Latn",
                    "hin-Deva"
                ],
                "accuracy": 83.97596394591888,
                "f1": 79.85502062617736,
                "main_score": 79.85502062617736,
                "precision": 78.01758192844824,
                "recall": 83.97596394591888
            },
            {
                "hf_subset": "hun_Latn-ind_Latn",
                "languages": [
                    "hun-Latn",
                    "ind-Latn"
                ],
                "accuracy": 92.68903355032549,
                "f1": 90.64596895343014,
                "main_score": 90.64596895343014,
                "precision": 89.68869971624103,
                "recall": 92.68903355032549
            },
            {
                "hf_subset": "hun_Latn-jpn_Jpan",
                "languages": [
                    "hun-Latn",
                    "jpn-Jpan"
                ],
                "accuracy": 85.778668002003,
                "f1": 82.19829744616925,
                "main_score": 82.19829744616925,
                "precision": 80.62426973794025,
                "recall": 85.778668002003
            },
            {
                "hf_subset": "hun_Latn-kor_Hang",
                "languages": [
                    "hun-Latn",
                    "kor-Hang"
                ],
                "accuracy": 84.17626439659489,
                "f1": 80.26746468909714,
                "main_score": 80.26746468909714,
                "precision": 78.5646097351155,
                "recall": 84.17626439659489
            },
            {
                "hf_subset": "hun_Latn-lav_Latn",
                "languages": [
                    "hun-Latn",
                    "lav-Latn"
                ],
                "accuracy": 90.1352028042063,
                "f1": 87.30262059756302,
                "main_score": 87.30262059756302,
                "precision": 85.98731430479052,
                "recall": 90.1352028042063
            },
            {
                "hf_subset": "hun_Latn-lit_Latn",
                "languages": [
                    "hun-Latn",
                    "lit-Latn"
                ],
                "accuracy": 89.58437656484726,
                "f1": 86.8252378567852,
                "main_score": 86.8252378567852,
                "precision": 85.54581872809214,
                "recall": 89.58437656484726
            },
            {
                "hf_subset": "hun_Latn-nld_Latn",
                "languages": [
                    "hun-Latn",
                    "nld-Latn"
                ],
                "accuracy": 93.03955933900852,
                "f1": 91.03989317309296,
                "main_score": 91.03989317309296,
                "precision": 90.08930061759305,
                "recall": 93.03955933900852
            },
            {
                "hf_subset": "hun_Latn-pol_Latn",
                "languages": [
                    "hun-Latn",
                    "pol-Latn"
                ],
                "accuracy": 91.58738107160741,
                "f1": 89.28225671841095,
                "main_score": 89.28225671841095,
                "precision": 88.18227341011517,
                "recall": 91.58738107160741
            },
            {
                "hf_subset": "hun_Latn-por_Latn",
                "languages": [
                    "hun-Latn",
                    "por-Latn"
                ],
                "accuracy": 93.59038557836755,
                "f1": 91.71256885327992,
                "main_score": 91.71256885327992,
                "precision": 90.80287097312635,
                "recall": 93.59038557836755
            },
            {
                "hf_subset": "hun_Latn-rus_Cyrl",
                "languages": [
                    "hun-Latn",
                    "rus-Cyrl"
                ],
                "accuracy": 91.3370055082624,
                "f1": 88.88916708395926,
                "main_score": 88.88916708395926,
                "precision": 87.75961561389704,
                "recall": 91.3370055082624
            },
            {
                "hf_subset": "hun_Latn-spa_Latn",
                "languages": [
                    "hun-Latn",
                    "spa-Latn"
                ],
                "accuracy": 93.69053580370556,
                "f1": 91.94959105324652,
                "main_score": 91.94959105324652,
                "precision": 91.12418627941913,
                "recall": 93.69053580370556
            },
            {
                "hf_subset": "hun_Latn-swa_Latn",
                "languages": [
                    "hun-Latn",
                    "swa-Latn"
                ],
                "accuracy": 35.803705558337505,
                "f1": 27.79832969518814,
                "main_score": 27.79832969518814,
                "precision": 25.370895920971037,
                "recall": 35.803705558337505
            },
            {
                "hf_subset": "hun_Latn-swe_Latn",
                "languages": [
                    "hun-Latn",
                    "swe-Latn"
                ],
                "accuracy": 93.59038557836755,
                "f1": 91.66249374061091,
                "main_score": 91.66249374061091,
                "precision": 90.74445000834585,
                "recall": 93.59038557836755
            },
            {
                "hf_subset": "hun_Latn-tam_Taml",
                "languages": [
                    "hun-Latn",
                    "tam-Taml"
                ],
                "accuracy": 27.391086629944915,
                "f1": 19.094552675413095,
                "main_score": 19.094552675413095,
                "precision": 16.88288208814635,
                "recall": 27.391086629944915
            },
            {
                "hf_subset": "hun_Latn-tur_Latn",
                "languages": [
                    "hun-Latn",
                    "tur-Latn"
                ],
                "accuracy": 91.48723084626941,
                "f1": 89.11700884660323,
                "main_score": 89.11700884660323,
                "precision": 87.99031881155067,
                "recall": 91.48723084626941
            },
            {
                "hf_subset": "hun_Latn-vie_Latn",
                "languages": [
                    "hun-Latn",
                    "vie-Latn"
                ],
                "accuracy": 91.13670505758637,
                "f1": 88.6696711734268,
                "main_score": 88.6696711734268,
                "precision": 87.49374061091638,
                "recall": 91.13670505758637
            },
            {
                "hf_subset": "hun_Latn-zho_Hant",
                "languages": [
                    "hun-Latn",
                    "zho-Hant"
                ],
                "accuracy": 89.33400100150224,
                "f1": 86.55745523046474,
                "main_score": 86.55745523046474,
                "precision": 85.29794692038057,
                "recall": 89.33400100150224
            },
            {
                "hf_subset": "hun_Latn-zul_Latn",
                "languages": [
                    "hun-Latn",
                    "zul-Latn"
                ],
                "accuracy": 16.675012518778168,
                "f1": 11.21636405139599,
                "main_score": 11.21636405139599,
                "precision": 9.903070059112947,
                "recall": 16.675012518778168
            },
            {
                "hf_subset": "ind_Latn-hun_Latn",
                "languages": [
                    "ind-Latn",
                    "hun-Latn"
                ],
                "accuracy": 92.93940911367051,
                "f1": 90.96478050408946,
                "main_score": 90.96478050408946,
                "precision": 90.03922550492406,
                "recall": 92.93940911367051
            },
            {
                "hf_subset": "jpn_Jpan-hun_Latn",
                "languages": [
                    "jpn-Jpan",
                    "hun-Latn"
                ],
                "accuracy": 88.28242363545317,
                "f1": 85.11433817392756,
                "main_score": 85.11433817392756,
                "precision": 83.67551326990485,
                "recall": 88.28242363545317
            },
            {
                "hf_subset": "kor_Hang-hun_Latn",
                "languages": [
                    "kor-Hang",
                    "hun-Latn"
                ],
                "accuracy": 85.778668002003,
                "f1": 81.83608746453012,
                "main_score": 81.83608746453012,
                "precision": 80.0233683859122,
                "recall": 85.778668002003
            },
            {
                "hf_subset": "lav_Latn-hun_Latn",
                "languages": [
                    "lav-Latn",
                    "hun-Latn"
                ],
                "accuracy": 91.73760640961443,
                "f1": 89.42914371557336,
                "main_score": 89.42914371557336,
                "precision": 88.32832582206642,
                "recall": 91.73760640961443
            },
            {
                "hf_subset": "lit_Latn-hun_Latn",
                "languages": [
                    "lit-Latn",
                    "hun-Latn"
                ],
                "accuracy": 91.78768152228342,
                "f1": 89.50926389584376,
                "main_score": 89.50926389584376,
                "precision": 88.39926556501419,
                "recall": 91.78768152228342
            },
            {
                "hf_subset": "nld_Latn-hun_Latn",
                "languages": [
                    "nld-Latn",
                    "hun-Latn"
                ],
                "accuracy": 93.49023535302955,
                "f1": 91.6190953096311,
                "main_score": 91.6190953096311,
                "precision": 90.72775830412286,
                "recall": 93.49023535302955
            },
            {
                "hf_subset": "pol_Latn-hun_Latn",
                "languages": [
                    "pol-Latn",
                    "hun-Latn"
                ],
                "accuracy": 91.28693039559339,
                "f1": 88.99515940577533,
                "main_score": 88.99515940577533,
                "precision": 87.9293940911367,
                "recall": 91.28693039559339
            },
            {
                "hf_subset": "por_Latn-hun_Latn",
                "languages": [
                    "por-Latn",
                    "hun-Latn"
                ],
                "accuracy": 93.03955933900852,
                "f1": 91.08496077449509,
                "main_score": 91.08496077449509,
                "precision": 90.17860123518612,
                "recall": 93.03955933900852
            },
            {
                "hf_subset": "rus_Cyrl-hun_Latn",
                "languages": [
                    "rus-Cyrl",
                    "hun-Latn"
                ],
                "accuracy": 90.98647971957938,
                "f1": 88.43932565514937,
                "main_score": 88.43932565514937,
                "precision": 87.2475379736271,
                "recall": 90.98647971957938
            },
            {
                "hf_subset": "spa_Latn-hun_Latn",
                "languages": [
                    "spa-Latn",
                    "hun-Latn"
                ],
                "accuracy": 93.23985978968453,
                "f1": 91.3386746786847,
                "main_score": 91.3386746786847,
                "precision": 90.43148055416457,
                "recall": 93.23985978968453
            },
            {
                "hf_subset": "swa_Latn-hun_Latn",
                "languages": [
                    "swa-Latn",
                    "hun-Latn"
                ],
                "accuracy": 35.95393089634452,
                "f1": 30.612257939034187,
                "main_score": 30.612257939034187,
                "precision": 28.995078568906944,
                "recall": 35.95393089634452
            },
            {
                "hf_subset": "swe_Latn-hun_Latn",
                "languages": [
                    "swe-Latn",
                    "hun-Latn"
                ],
                "accuracy": 93.64046069103655,
                "f1": 91.86613253213153,
                "main_score": 91.86613253213153,
                "precision": 91.04072775830413,
                "recall": 93.64046069103655
            },
            {
                "hf_subset": "tam_Taml-hun_Latn",
                "languages": [
                    "tam-Taml",
                    "hun-Latn"
                ],
                "accuracy": 29.04356534802203,
                "f1": 25.164093122029808,
                "main_score": 25.164093122029808,
                "precision": 23.849573878565543,
                "recall": 29.04356534802203
            },
            {
                "hf_subset": "tur_Latn-hun_Latn",
                "languages": [
                    "tur-Latn",
                    "hun-Latn"
                ],
                "accuracy": 90.83625438157236,
                "f1": 88.36087464530128,
                "main_score": 88.36087464530128,
                "precision": 87.19829744616925,
                "recall": 90.83625438157236
            },
            {
                "hf_subset": "vie_Latn-hun_Latn",
                "languages": [
                    "vie-Latn",
                    "hun-Latn"
                ],
                "accuracy": 90.68602904356536,
                "f1": 88.10882991153397,
                "main_score": 88.10882991153397,
                "precision": 86.90118511099983,
                "recall": 90.68602904356536
            },
            {
                "hf_subset": "zho_Hant-hun_Latn",
                "languages": [
                    "zho-Hant",
                    "hun-Latn"
                ],
                "accuracy": 90.1352028042063,
                "f1": 87.46035720247039,
                "main_score": 87.46035720247039,
                "precision": 86.19810668383528,
                "recall": 90.1352028042063
            },
            {
                "hf_subset": "zul_Latn-hun_Latn",
                "languages": [
                    "zul-Latn",
                    "hun-Latn"
                ],
                "accuracy": 17.1256885327992,
                "f1": 13.692538409811572,
                "main_score": 13.692538409811572,
                "precision": 12.811084017018844,
                "recall": 17.1256885327992
            }
        ]
    }
}