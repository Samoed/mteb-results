{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": NaN,
    "mteb_version": "unknown",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 64.05850706119705,
                "f1": 62.20100273658395,
                "main_score": 64.05850706119705
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 63.24142568930733,
                "f1": 62.045023522098205,
                "main_score": 63.24142568930733
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 63.685272360457304,
                "f1": 63.315744557403285,
                "main_score": 63.685272360457304
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 60.85743106926698,
                "f1": 59.106917986505636,
                "main_score": 60.85743106926698
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 67.1654337592468,
                "f1": 65.66986920813582,
                "main_score": 67.1654337592468
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 56.519838601210495,
                "f1": 54.73278620356587,
                "main_score": 56.519838601210495
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 67.76395427034298,
                "f1": 66.3447645997219,
                "main_score": 67.76395427034298
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 67.47814391392065,
                "f1": 66.32841368787447,
                "main_score": 67.47814391392065
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 70.22864828513787,
                "f1": 69.02774052818218,
                "main_score": 70.22864828513787
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 69.04841963685273,
                "f1": 67.70789401248665,
                "main_score": 69.04841963685273
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 69.08204438466711,
                "f1": 68.39277940460933,
                "main_score": 69.08204438466711
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 72.10154673839946,
                "f1": 70.7737194288215,
                "main_score": 72.10154673839946
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 67.16207128446537,
                "f1": 66.2311820377212,
                "main_score": 67.16207128446537
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 63.019502353732335,
                "f1": 62.105500895318656,
                "main_score": 63.019502353732335
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 68.82985877605918,
                "f1": 67.4894449433449,
                "main_score": 68.82985877605918
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 66.89643577673168,
                "f1": 65.45745898521055,
                "main_score": 66.89643577673168
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 69.32750504371216,
                "f1": 68.19665323990438,
                "main_score": 69.32750504371216
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 65.8238063214526,
                "f1": 64.60872984606974,
                "main_score": 65.8238063214526
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 68.98117014122394,
                "f1": 67.66697147027641,
                "main_score": 68.98117014122394
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 63.137188971082715,
                "f1": 61.58358081191463,
                "main_score": 63.137188971082715
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 70.0437121721587,
                "f1": 69.06747206775307,
                "main_score": 70.0437121721587
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 70.67585743106926,
                "f1": 70.08618915891508,
                "main_score": 70.67585743106926
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 59.788164088769335,
                "f1": 57.91398932676417,
                "main_score": 59.788164088769335
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 61.03227975790182,
                "f1": 60.044432258486715,
                "main_score": 61.03227975790182
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 49.051782111634154,
                "f1": 45.434581931581555,
                "main_score": 49.051782111634154
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 63.78278412911902,
                "f1": 62.106197625881535,
                "main_score": 63.78278412911902
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 69.59986550100874,
                "f1": 68.94355682848476,
                "main_score": 69.59986550100874
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 59.97310020174847,
                "f1": 59.09912773329623,
                "main_score": 59.97310020174847
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 69.20309347679893,
                "f1": 67.90665916607239,
                "main_score": 69.20309347679893
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 62.72024209818427,
                "f1": 60.77165334831407,
                "main_score": 62.72024209818427
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 67.87155346334902,
                "f1": 65.7906032446679,
                "main_score": 67.87155346334902
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 64.97646267652992,
                "f1": 63.89390215791396,
                "main_score": 64.97646267652992
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 65.81371889710827,
                "f1": 64.39323436519936,
                "main_score": 65.81371889710827
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 69.79825151311366,
                "f1": 68.53789900442244,
                "main_score": 69.79825151311366
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 69.98991257565568,
                "f1": 68.93867074879778,
                "main_score": 69.98991257565568
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 67.50168123739071,
                "f1": 66.7457644903972,
                "main_score": 67.50168123739071
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 67.52521856086078,
                "f1": 66.83370797374445,
                "main_score": 67.52521856086078
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 67.96234028244787,
                "f1": 67.58983110064196,
                "main_score": 67.96234028244787
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 69.56624075319435,
                "f1": 68.35270162147211,
                "main_score": 69.56624075319435
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 68.48352387357095,
                "f1": 66.66973143886908,
                "main_score": 68.48352387357095
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 67.92535305985206,
                "f1": 66.52058462942483,
                "main_score": 67.92535305985206
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 63.184263618022875,
                "f1": 61.71153164960602,
                "main_score": 63.184263618022875
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 64.8453261600538,
                "f1": 63.863209439112346,
                "main_score": 64.8453261600538
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 65.39340954942838,
                "f1": 63.85484524633183,
                "main_score": 65.39340954942838
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 67.9892400806994,
                "f1": 66.57022479007357,
                "main_score": 67.9892400806994
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 63.399462004034966,
                "f1": 61.62381473991175,
                "main_score": 63.399462004034966
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 65.773369199731,
                "f1": 65.58317907780943,
                "main_score": 65.773369199731
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 65.8069939475454,
                "f1": 64.47027323557235,
                "main_score": 65.8069939475454
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 66.51647612642904,
                "f1": 65.66061210324213,
                "main_score": 66.51647612642904
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 68.88365837256221,
                "f1": 67.56956454874091,
                "main_score": 68.88365837256221
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 63.29858776059179,
                "f1": 62.76318771484755,
                "main_score": 63.29858776059179
            }
        ]
    }
}