{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": NaN,
    "mteb_version": "unknown",
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.9,
                "f1": 98.55000000000001,
                "precision": 98.38333333333334,
                "recall": 98.9,
                "main_score": 98.55000000000001
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 65.3179190751445,
                "f1": 59.44582071749702,
                "precision": 57.49678869621066,
                "recall": 65.3179190751445,
                "main_score": 59.44582071749702
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 38.53658536585366,
                "f1": 34.217555952803785,
                "precision": 32.96511296649355,
                "recall": 38.53658536585366,
                "main_score": 34.217555952803785
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.7,
                "f1": 98.26666666666665,
                "precision": 98.05,
                "recall": 98.7,
                "main_score": 98.26666666666665
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 99.3,
                "f1": 99.13333333333333,
                "precision": 99.05000000000001,
                "recall": 99.3,
                "main_score": 99.13333333333333
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.89999999999999,
                "f1": 97.2,
                "precision": 96.85000000000001,
                "recall": 97.89999999999999,
                "main_score": 97.2
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.2,
                "f1": 97.6,
                "precision": 97.3,
                "recall": 98.2,
                "main_score": 97.6
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 52.23880597014925,
                "f1": 46.340992406389105,
                "precision": 44.556384742951906,
                "recall": 52.23880597014925,
                "main_score": 46.340992406389105
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.0,
                "f1": 93.67000000000002,
                "precision": 93.075,
                "recall": 95.0,
                "main_score": 93.67000000000002
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 88.29268292682927,
                "f1": 85.76422764227642,
                "precision": 84.84204413472706,
                "recall": 88.29268292682927,
                "main_score": 85.76422764227642
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.2,
                "f1": 96.46666666666667,
                "precision": 96.1,
                "recall": 97.2,
                "main_score": 96.46666666666667
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.8408262454435,
                "f1": 95.9902794653706,
                "precision": 95.56500607533415,
                "recall": 96.8408262454435,
                "main_score": 95.9902794653706
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.3913043478261,
                "f1": 91.30434782608695,
                "precision": 90.28985507246377,
                "recall": 93.3913043478261,
                "main_score": 91.30434782608695
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 90.6086956521739,
                "f1": 88.1159420289855,
                "precision": 86.9623188405797,
                "recall": 90.6086956521739,
                "main_score": 88.1159420289855
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.16666666666667,
                "precision": 96.86666666666667,
                "recall": 97.8,
                "main_score": 97.16666666666667
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 94.0,
                "f1": 92.34,
                "precision": 91.54166666666667,
                "recall": 94.0,
                "main_score": 92.34
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 84.92159227985525,
                "f1": 80.8868975817106,
                "precision": 79.11540008041817,
                "recall": 84.92159227985525,
                "main_score": 80.8868975817106
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 94.89999999999999,
                "f1": 93.35,
                "precision": 92.58333333333334,
                "recall": 94.89999999999999,
                "main_score": 93.35
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 43.3,
                "f1": 36.64473116255726,
                "precision": 34.64017752457381,
                "recall": 43.3,
                "main_score": 36.64473116255726
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 96.7,
                "f1": 95.68333333333332,
                "precision": 95.19999999999999,
                "recall": 96.7,
                "main_score": 95.68333333333332
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 70.47619047619048,
                "f1": 66.63032734461306,
                "precision": 65.46459191863879,
                "recall": 70.47619047619048,
                "main_score": 66.63032734461306
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.5,
                "f1": 91.63,
                "precision": 90.75,
                "recall": 93.5,
                "main_score": 91.63
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 95.5,
                "f1": 94.36666666666666,
                "precision": 93.83333333333333,
                "recall": 95.5,
                "main_score": 94.36666666666666
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 99.3,
                "f1": 99.06666666666666,
                "precision": 98.95,
                "recall": 99.3,
                "main_score": 99.06666666666666
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.8,
                "f1": 94.51666666666667,
                "precision": 93.88333333333334,
                "recall": 95.8,
                "main_score": 94.51666666666667
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 84.0,
                "f1": 80.46675324675326,
                "precision": 78.95999999999998,
                "recall": 84.0,
                "main_score": 80.46675324675326
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.7,
                "f1": 96.93333333333332,
                "precision": 96.55,
                "recall": 97.7,
                "main_score": 96.93333333333332
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 92.10000000000001,
                "f1": 90.07333333333334,
                "precision": 89.16166666666668,
                "recall": 92.10000000000001,
                "main_score": 90.07333333333334
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.6,
                "f1": 94.35,
                "precision": 93.75,
                "recall": 95.6,
                "main_score": 94.35
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.9,
                "f1": 98.53333333333335,
                "precision": 98.35000000000001,
                "recall": 98.9,
                "main_score": 98.53333333333335
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 96.22641509433963,
                "f1": 95.14824797843666,
                "precision": 94.60916442048517,
                "recall": 96.22641509433963,
                "main_score": 95.14824797843666
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 93.58974358974359,
                "f1": 91.59544159544159,
                "precision": 90.66951566951566,
                "recall": 93.58974358974359,
                "main_score": 91.59544159544159
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.1,
                "f1": 97.46666666666668,
                "precision": 97.15,
                "recall": 98.1,
                "main_score": 97.46666666666668
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 93.4090909090909,
                "f1": 91.5909090909091,
                "precision": 90.71969696969697,
                "recall": 93.4090909090909,
                "main_score": 91.5909090909091
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 89.51781970649894,
                "f1": 86.76150544075072,
                "precision": 85.55206149545772,
                "recall": 89.51781970649894,
                "main_score": 86.76150544075072
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.2,
                "f1": 97.65,
                "precision": 97.38333333333333,
                "recall": 98.2,
                "main_score": 97.65
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 75.87548638132296,
                "f1": 71.24698906800073,
                "precision": 69.66572338167668,
                "recall": 75.87548638132296,
                "main_score": 71.24698906800073
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 61.53846153846154,
                "f1": 54.83234714003944,
                "precision": 52.06552706552707,
                "recall": 61.53846153846154,
                "main_score": 54.83234714003944
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 59.199999999999996,
                "f1": 54.183211233211225,
                "precision": 52.48751719986241,
                "recall": 59.199999999999996,
                "main_score": 54.183211233211225
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.6,
                "f1": 94.3,
                "precision": 93.65,
                "recall": 95.6,
                "main_score": 94.3
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 87.85046728971963,
                "f1": 85.25700934579439,
                "precision": 84.09267912772586,
                "recall": 87.85046728971963,
                "main_score": 85.25700934579439
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.0,
                "f1": 97.43333333333332,
                "precision": 97.15,
                "recall": 98.0,
                "main_score": 97.43333333333332
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.8,
                "f1": 88.66055555555555,
                "precision": 87.81845238095238,
                "recall": 90.8,
                "main_score": 88.66055555555555
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 70.6,
                "f1": 65.538895353013,
                "precision": 63.69531394330308,
                "recall": 70.6,
                "main_score": 65.538895353013
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.89999999999999,
                "f1": 96.06666666666668,
                "precision": 95.68333333333334,
                "recall": 96.89999999999999,
                "main_score": 96.06666666666668
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.8,
                "f1": 95.95,
                "precision": 95.55,
                "recall": 96.8,
                "main_score": 95.95
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 93.8,
                "precision": 93.13333333333334,
                "recall": 95.19999999999999,
                "main_score": 93.8
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.5,
                "f1": 95.45,
                "precision": 94.93333333333334,
                "recall": 96.5,
                "main_score": 95.45
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.89999999999999,
                "f1": 97.28333333333332,
                "precision": 96.98333333333333,
                "recall": 97.89999999999999,
                "main_score": 97.28333333333332
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 78.16666666666666,
                "f1": 74.67336721249764,
                "precision": 73.26035353535354,
                "recall": 78.16666666666666,
                "main_score": 74.67336721249764
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.200000000000001,
                "f1": 8.48123815073815,
                "precision": 7.843657708032708,
                "recall": 11.200000000000001,
                "main_score": 8.48123815073815
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 91.3,
                "f1": 89.02333333333333,
                "precision": 87.97500000000001,
                "recall": 91.3,
                "main_score": 89.02333333333333
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 72.32142857142857,
                "f1": 67.69209956709956,
                "precision": 66.19047619047619,
                "recall": 72.32142857142857,
                "main_score": 67.69209956709956
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 79.69264544456641,
                "f1": 75.40693115885212,
                "precision": 73.67544822539335,
                "recall": 79.69264544456641,
                "main_score": 75.40693115885212
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 86.8,
                "f1": 83.65666666666667,
                "precision": 82.24833333333333,
                "recall": 86.8,
                "main_score": 83.65666666666667
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.39999999999999,
                "f1": 95.36666666666666,
                "precision": 94.86666666666666,
                "recall": 96.39999999999999,
                "main_score": 95.36666666666666
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.3,
                "f1": 95.49,
                "precision": 95.10833333333333,
                "recall": 96.3,
                "main_score": 95.49
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 89.60000000000001,
                "f1": 87.04746031746032,
                "precision": 85.89583333333333,
                "recall": 89.60000000000001,
                "main_score": 87.04746031746032
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 86.9,
                "f1": 84.57088023088022,
                "precision": 83.6475,
                "recall": 86.9,
                "main_score": 84.57088023088022
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.2,
                "f1": 97.7,
                "precision": 97.46666666666668,
                "recall": 98.2,
                "main_score": 97.7
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 85.39999999999999,
                "f1": 82.83333333333333,
                "precision": 81.80137426900586,
                "recall": 85.39999999999999,
                "main_score": 82.83333333333333
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 91.4,
                "f1": 89.11999999999999,
                "precision": 88.12777777777778,
                "recall": 91.4,
                "main_score": 89.11999999999999
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.16666666666669,
                "precision": 96.85000000000001,
                "recall": 97.8,
                "main_score": 97.16666666666669
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.89999999999999,
                "f1": 97.30666666666666,
                "precision": 97.02499999999999,
                "recall": 97.89999999999999,
                "main_score": 97.30666666666666
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 27.00729927007299,
                "f1": 25.114895917815623,
                "precision": 24.602283361407448,
                "recall": 27.00729927007299,
                "main_score": 25.114895917815623
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 14.099999999999998,
                "f1": 11.869284007509814,
                "precision": 11.199695454818405,
                "recall": 14.099999999999998,
                "main_score": 11.869284007509814
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.7,
                "f1": 97.09,
                "precision": 96.80833333333332,
                "recall": 97.7,
                "main_score": 97.09
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 96.5,
                "f1": 95.47333333333333,
                "precision": 94.975,
                "recall": 96.5,
                "main_score": 95.47333333333333
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 93.45238095238095,
                "f1": 91.66666666666666,
                "precision": 90.77380952380952,
                "recall": 93.45238095238095,
                "main_score": 91.66666666666666
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 11.899999999999999,
                "f1": 10.303261315113037,
                "precision": 9.902986584515606,
                "recall": 11.899999999999999,
                "main_score": 10.303261315113037
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 81.57349896480332,
                "f1": 77.86519438693352,
                "precision": 76.35595081247254,
                "recall": 81.57349896480332,
                "main_score": 77.86519438693352
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 96.1,
                "f1": 94.86666666666667,
                "precision": 94.25,
                "recall": 96.1,
                "main_score": 94.86666666666667
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.8,
                "f1": 98.46666666666667,
                "precision": 98.3,
                "recall": 98.8,
                "main_score": 98.46666666666667
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 10.7,
                "f1": 8.621683883854935,
                "precision": 8.188292731521031,
                "recall": 10.7,
                "main_score": 8.621683883854935
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 90.47619047619048,
                "f1": 87.8581735724593,
                "precision": 86.72438672438673,
                "recall": 90.47619047619048,
                "main_score": 87.8581735724593
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.0381679389313,
                "f1": 93.60050890585242,
                "precision": 92.970737913486,
                "recall": 95.0381679389313,
                "main_score": 93.60050890585242
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 98.2532751091703,
                "f1": 97.67103347889375,
                "precision": 97.37991266375546,
                "recall": 98.2532751091703,
                "main_score": 97.67103347889375
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 84.6,
                "f1": 80.99904761904763,
                "precision": 79.54634920634919,
                "recall": 84.6,
                "main_score": 80.99904761904763
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.89265536723164,
                "f1": 95.90395480225989,
                "precision": 95.4331450094162,
                "recall": 96.89265536723164,
                "main_score": 95.90395480225989
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 12.6,
                "f1": 9.981918087824628,
                "precision": 9.326319147606549,
                "recall": 12.6,
                "main_score": 9.981918087824628
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.39999999999999,
                "f1": 96.65,
                "precision": 96.28333333333333,
                "recall": 97.39999999999999,
                "main_score": 96.65
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.5,
                "f1": 95.38333333333333,
                "precision": 94.83333333333333,
                "recall": 96.5,
                "main_score": 95.38333333333333
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 90.8,
                "f1": 88.43666666666665,
                "precision": 87.395,
                "recall": 90.8,
                "main_score": 88.43666666666665
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.7,
                "f1": 97.03333333333333,
                "precision": 96.71666666666667,
                "recall": 97.7,
                "main_score": 97.03333333333333
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 9.4,
                "f1": 7.946889105220061,
                "precision": 7.665059865752875,
                "recall": 9.4,
                "main_score": 7.946889105220061
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 95.04021447721179,
                "f1": 93.68632707774799,
                "precision": 93.08534405719392,
                "recall": 95.04021447721179,
                "main_score": 93.68632707774799
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 95.89999999999999,
                "f1": 94.66666666666667,
                "precision": 94.08333333333334,
                "recall": 95.89999999999999,
                "main_score": 94.66666666666667
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 82.6086956521739,
                "f1": 77.98418972332016,
                "precision": 75.96837944664031,
                "recall": 82.6086956521739,
                "main_score": 77.98418972332016
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.77464788732394,
                "f1": 94.8356807511737,
                "precision": 94.36619718309859,
                "recall": 95.77464788732394,
                "main_score": 94.8356807511737
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 53.17365269461077,
                "f1": 47.07043056743655,
                "precision": 45.161363241830784,
                "recall": 53.17365269461077,
                "main_score": 47.07043056743655
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.5,
                "f1": 94.5,
                "precision": 94.03333333333333,
                "recall": 95.5,
                "main_score": 94.5
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.59605911330048,
                "f1": 91.82266009852216,
                "precision": 91.09195402298852,
                "recall": 93.59605911330048,
                "main_score": 91.82266009852216
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 76.40845070422534,
                "f1": 72.73082942097027,
                "precision": 71.46686939820742,
                "recall": 76.40845070422534,
                "main_score": 72.73082942097027
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.58974358974359,
                "f1": 91.98290598290598,
                "precision": 91.3119658119658,
                "recall": 93.58974358974359,
                "main_score": 91.98290598290598
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.06666666666668,
                "precision": 96.7,
                "recall": 97.8,
                "main_score": 97.06666666666668
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 68.89352818371609,
                "f1": 64.47860652453555,
                "precision": 62.878651918592574,
                "recall": 68.89352818371609,
                "main_score": 64.47860652453555
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 33.800000000000004,
                "f1": 29.290774344112368,
                "precision": 28.066016735704647,
                "recall": 33.800000000000004,
                "main_score": 29.290774344112368
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 90.22801302931596,
                "f1": 88.07817589576547,
                "precision": 87.171552660152,
                "recall": 90.22801302931596,
                "main_score": 88.07817589576547
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.2,
                "f1": 97.63333333333334,
                "precision": 97.36666666666667,
                "recall": 98.2,
                "main_score": 97.63333333333334
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.7,
                "f1": 96.95,
                "precision": 96.58333333333331,
                "recall": 97.7,
                "main_score": 96.95
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.91338582677166,
                "f1": 90.81364829396327,
                "precision": 89.89501312335958,
                "recall": 92.91338582677166,
                "main_score": 90.81364829396327
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 96.89999999999999,
                "f1": 95.98333333333332,
                "precision": 95.56666666666668,
                "recall": 96.89999999999999,
                "main_score": 95.98333333333332
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 74.51523545706371,
                "f1": 70.20346919931407,
                "precision": 68.6389565788895,
                "recall": 74.51523545706371,
                "main_score": 70.20346919931407
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.6,
                "f1": 96.88333333333333,
                "precision": 96.53333333333333,
                "recall": 97.6,
                "main_score": 96.88333333333333
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 46.15384615384615,
                "f1": 39.47885447885448,
                "precision": 37.301528599605525,
                "recall": 46.15384615384615,
                "main_score": 39.47885447885448
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 94.69999999999999,
                "f1": 93.16666666666667,
                "precision": 92.41666666666667,
                "recall": 94.69999999999999,
                "main_score": 93.16666666666667
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 93.83333333333333,
                "precision": 93.16666666666667,
                "recall": 95.19999999999999,
                "main_score": 93.83333333333333
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 92.0,
                "f1": 89.98666666666666,
                "precision": 89.09166666666667,
                "recall": 92.0,
                "main_score": 89.98666666666666
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 95.51886792452831,
                "f1": 94.3003144654088,
                "precision": 93.75,
                "recall": 95.51886792452831,
                "main_score": 94.3003144654088
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.2,
                "f1": 97.83333333333333,
                "precision": 97.65,
                "recall": 98.2,
                "main_score": 97.83333333333333
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 96.8978102189781,
                "f1": 96.04622871046227,
                "precision": 95.62043795620438,
                "recall": 96.8978102189781,
                "main_score": 96.04622871046227
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 85.1,
                "f1": 81.78564213564214,
                "precision": 80.46416666666667,
                "recall": 85.1,
                "main_score": 81.78564213564214
            }
        ]
    }
}