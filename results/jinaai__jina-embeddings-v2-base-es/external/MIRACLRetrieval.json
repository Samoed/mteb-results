{
    "dataset_revision": "None",
    "task_name": "MIRACLRetrieval",
    "evaluation_time": NaN,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "None"
                ],
                "map_at_1": 21.299,
                "map_at_10": 70.547,
                "map_at_100": 72.394,
                "map_at_1000": 72.39999999999999,
                "map_at_3": 41.317,
                "map_at_5": 53.756,
                "mrr_at_1": 72.84,
                "mrr_at_10": 82.466,
                "mrr_at_100": 82.52199999999999,
                "mrr_at_1000": 82.52199999999999,
                "mrr_at_3": 80.607,
                "mrr_at_5": 82.065,
                "ndcg_at_1": 72.994,
                "ndcg_at_10": 80.89,
                "ndcg_at_100": 83.30199999999999,
                "ndcg_at_1000": 83.337,
                "ndcg_at_3": 70.357,
                "ndcg_at_5": 72.529,
                "precision_at_1": 72.994,
                "precision_at_10": 43.056,
                "precision_at_100": 4.603,
                "precision_at_1000": 0.461,
                "precision_at_3": 61.626000000000005,
                "precision_at_5": 55.525000000000006,
                "recall_at_1": 21.299,
                "recall_at_10": 93.903,
                "recall_at_100": 99.86699999999999,
                "recall_at_1000": 100.0,
                "recall_at_3": 46.653,
                "recall_at_5": 65.72200000000001,
                "main_score": 80.89
            }
        ]
    }
}