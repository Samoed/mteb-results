{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 73.33557498318764,
                "f1": 72.28949738478356,
                "main_score": 73.33557498318764
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 65.84398117014123,
                "f1": 64.71026362091463,
                "main_score": 65.84398117014123
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 69.76462676529925,
                "f1": 69.8229667407667,
                "main_score": 69.76462676529925
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 72.02420981842636,
                "f1": 71.76576384895898,
                "main_score": 72.02420981842636
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 72.7572293207801,
                "f1": 72.76840765295256,
                "main_score": 72.7572293207801
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 68.02286482851379,
                "f1": 66.17237947327872,
                "main_score": 68.02286482851379
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 77.60928043039678,
                "f1": 77.27094731234773,
                "main_score": 77.60928043039678
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 77.68325487558843,
                "f1": 77.97530399082261,
                "main_score": 77.68325487558843
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 76.13315400134498,
                "f1": 75.97558584796424,
                "main_score": 76.13315400134498
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 80.47410894418292,
                "f1": 80.52244841473792,
                "main_score": 80.47410894418292
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 76.9670477471419,
                "f1": 77.37318805793146,
                "main_score": 76.9670477471419
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 78.09683927370544,
                "f1": 77.69773737430847,
                "main_score": 78.09683927370544
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 75.20847343644922,
                "f1": 75.17071738727348,
                "main_score": 75.20847343644922
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 77.07464694014796,
                "f1": 77.16136207698571,
                "main_score": 77.07464694014796
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 73.53396099529255,
                "f1": 73.58296404484122,
                "main_score": 73.53396099529255
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 75.75319435104237,
                "f1": 75.24674707850833,
                "main_score": 75.75319435104237
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 77.0948217888366,
                "f1": 76.47559490205028,
                "main_score": 77.0948217888366
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 71.07599193006052,
                "f1": 70.76028043093511,
                "main_score": 71.07599193006052
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 77.10490921318089,
                "f1": 77.01215275283272,
                "main_score": 77.10490921318089
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 71.25756556825824,
                "f1": 70.20605314648762,
                "main_score": 71.25756556825824
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 77.08137188971082,
                "f1": 77.3899269057439,
                "main_score": 77.08137188971082
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 79.35440484196369,
                "f1": 79.58964690002772,
                "main_score": 79.35440484196369
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 68.42299932750504,
                "f1": 68.07844356925413,
                "main_score": 68.42299932750504
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 66.15669132481507,
                "f1": 65.89383352608513,
                "main_score": 66.15669132481507
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 60.11432414256894,
                "f1": 57.69910594559806,
                "main_score": 60.11432414256894
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 71.24747814391392,
                "f1": 70.42455553830918,
                "main_score": 71.24747814391392
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 76.46267652992603,
                "f1": 76.8854559308316,
                "main_score": 76.46267652992603
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 73.24815063887021,
                "f1": 72.77805034658074,
                "main_score": 73.24815063887021
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 74.11566913248151,
                "f1": 73.86147988001356,
                "main_score": 74.11566913248151
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 70.0168123739072,
                "f1": 69.38515920054571,
                "main_score": 70.0168123739072
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 74.41156691324814,
                "f1": 73.43474953408237,
                "main_score": 74.41156691324814
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 68.39609952925353,
                "f1": 67.29731681109291,
                "main_score": 68.39609952925353
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 77.20914593140552,
                "f1": 77.07066497935367,
                "main_score": 77.20914593140552
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 78.52387357094821,
                "f1": 78.5259569473291,
                "main_score": 78.52387357094821
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 76.6913248150639,
                "f1": 76.91201656350455,
                "main_score": 76.6913248150639
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 77.1217215870881,
                "f1": 77.41179937912504,
                "main_score": 77.1217215870881
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 75.25891055817083,
                "f1": 75.8089244542887,
                "main_score": 75.25891055817083
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 77.70679219905851,
                "f1": 78.21459594517711,
                "main_score": 77.70679219905851
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 74.83523873570948,
                "f1": 74.86847028401978,
                "main_score": 74.83523873570948
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 74.71755211835911,
                "f1": 74.0214326485662,
                "main_score": 74.71755211835911
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 79.06523201075991,
                "f1": 79.10545620325138,
                "main_score": 79.06523201075991
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 67.91862811028918,
                "f1": 66.50386121217983,
                "main_score": 67.91862811028918
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 70.93140551445865,
                "f1": 70.755435928495,
                "main_score": 70.93140551445865
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 72.40753194351042,
                "f1": 71.61816115782923,
                "main_score": 72.40753194351042
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 75.1815736381977,
                "f1": 75.08016717887205,
                "main_score": 75.1815736381977
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 72.86482851378614,
                "f1": 72.39521180006291,
                "main_score": 72.86482851378614
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 76.46940147948891,
                "f1": 76.70044085362349,
                "main_score": 76.46940147948891
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 71.89307330195024,
                "f1": 71.5721825332298,
                "main_score": 71.89307330195024
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 74.7511768661735,
                "f1": 75.17918654541515,
                "main_score": 74.7511768661735
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 78.69535978480162,
                "f1": 78.90019070153316,
                "main_score": 78.69535978480162
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 75.45729657027572,
                "f1": 76.19578371794672,
                "main_score": 75.45729657027572
            }
        ]
    }
}