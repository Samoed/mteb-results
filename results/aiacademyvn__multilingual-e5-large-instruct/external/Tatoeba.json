{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.39999999999999,
                "f1": 96.75,
                "precision": 96.45,
                "recall": 97.39999999999999,
                "main_score": 96.75
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 85.54913294797689,
                "f1": 82.46628131021194,
                "precision": 81.1175337186898,
                "recall": 85.54913294797689,
                "main_score": 82.46628131021194
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 81.21951219512195,
                "f1": 77.33333333333334,
                "precision": 75.54878048780488,
                "recall": 81.21951219512195,
                "main_score": 77.33333333333334
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.6,
                "f1": 98.26666666666665,
                "precision": 98.1,
                "recall": 98.6,
                "main_score": 98.26666666666665
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 99.5,
                "f1": 99.33333333333333,
                "precision": 99.25,
                "recall": 99.5,
                "main_score": 99.33333333333333
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.2,
                "precision": 96.89999999999999,
                "recall": 97.8,
                "main_score": 97.2
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.18333333333334,
                "precision": 96.88333333333333,
                "recall": 97.8,
                "main_score": 97.18333333333334
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 77.61194029850746,
                "f1": 72.81094527363183,
                "precision": 70.83333333333333,
                "recall": 77.61194029850746,
                "main_score": 72.81094527363183
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.7,
                "f1": 91.91666666666667,
                "precision": 91.08333333333334,
                "recall": 93.7,
                "main_score": 91.91666666666667
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 88.29268292682927,
                "f1": 85.27642276422765,
                "precision": 84.01277584204414,
                "recall": 88.29268292682927,
                "main_score": 85.27642276422765
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.1,
                "f1": 95.0,
                "precision": 94.46666666666668,
                "recall": 96.1,
                "main_score": 95.0
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.681652490887,
                "f1": 91.90765492102065,
                "precision": 91.05913325232888,
                "recall": 93.681652490887,
                "main_score": 91.90765492102065
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.17391304347827,
                "f1": 89.97101449275361,
                "precision": 88.96811594202899,
                "recall": 92.17391304347827,
                "main_score": 89.97101449275361
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 90.43478260869566,
                "f1": 87.72173913043478,
                "precision": 86.42028985507245,
                "recall": 90.43478260869566,
                "main_score": 87.72173913043478
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.4,
                "f1": 88.03,
                "precision": 86.95,
                "recall": 90.4,
                "main_score": 88.03
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 93.4,
                "f1": 91.45666666666666,
                "precision": 90.525,
                "recall": 93.4,
                "main_score": 91.45666666666666
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 81.9059107358263,
                "f1": 78.32557872364869,
                "precision": 76.78260286824823,
                "recall": 81.9059107358263,
                "main_score": 78.32557872364869
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 94.3,
                "f1": 92.58333333333333,
                "precision": 91.73333333333332,
                "recall": 94.3,
                "main_score": 92.58333333333333
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 79.10000000000001,
                "f1": 74.50500000000001,
                "precision": 72.58928571428571,
                "recall": 79.10000000000001,
                "main_score": 74.50500000000001
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 96.6,
                "f1": 95.55,
                "precision": 95.05,
                "recall": 96.6,
                "main_score": 95.55
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 82.0952380952381,
                "f1": 77.98458049886621,
                "precision": 76.1968253968254,
                "recall": 82.0952380952381,
                "main_score": 77.98458049886621
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 87.9,
                "f1": 84.99190476190476,
                "precision": 83.65,
                "recall": 87.9,
                "main_score": 84.99190476190476
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 95.7,
                "f1": 94.56666666666666,
                "precision": 94.01666666666667,
                "recall": 95.7,
                "main_score": 94.56666666666666
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.6,
                "f1": 98.2,
                "precision": 98.0,
                "recall": 98.6,
                "main_score": 98.2
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.6,
                "f1": 94.38333333333334,
                "precision": 93.78333333333335,
                "recall": 95.6,
                "main_score": 94.38333333333334
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 87.4,
                "f1": 84.10380952380952,
                "precision": 82.67,
                "recall": 87.4,
                "main_score": 84.10380952380952
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.5,
                "f1": 94.33333333333334,
                "precision": 93.78333333333333,
                "recall": 95.5,
                "main_score": 94.33333333333334
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 89.4,
                "f1": 86.82000000000001,
                "precision": 85.64500000000001,
                "recall": 89.4,
                "main_score": 86.82000000000001
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.1,
                "f1": 93.56666666666668,
                "precision": 92.81666666666666,
                "recall": 95.1,
                "main_score": 93.56666666666668
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.9,
                "f1": 98.6,
                "precision": 98.45,
                "recall": 98.9,
                "main_score": 98.6
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 95.01347708894879,
                "f1": 93.51752021563343,
                "precision": 92.82794249775381,
                "recall": 95.01347708894879,
                "main_score": 93.51752021563343
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 97.00854700854701,
                "f1": 96.08262108262107,
                "precision": 95.65527065527067,
                "recall": 97.00854700854701,
                "main_score": 96.08262108262107
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.5,
                "f1": 95.39999999999999,
                "precision": 94.88333333333333,
                "recall": 96.5,
                "main_score": 95.39999999999999
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 96.5909090909091,
                "f1": 95.49242424242425,
                "precision": 94.9621212121212,
                "recall": 96.5909090909091,
                "main_score": 95.49242424242425
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 84.90566037735849,
                "f1": 81.85883997204752,
                "precision": 80.54507337526205,
                "recall": 84.90566037735849,
                "main_score": 81.85883997204752
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.5,
                "f1": 96.75,
                "precision": 96.38333333333333,
                "recall": 97.5,
                "main_score": 96.75
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 86.7704280155642,
                "f1": 82.99610894941635,
                "precision": 81.32295719844358,
                "recall": 86.7704280155642,
                "main_score": 82.99610894941635
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 67.52136752136752,
                "f1": 61.89662189662191,
                "precision": 59.68660968660969,
                "recall": 67.52136752136752,
                "main_score": 61.89662189662191
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 89.2,
                "f1": 86.32,
                "precision": 85.015,
                "recall": 89.2,
                "main_score": 86.32
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 96.0,
                "f1": 94.78333333333333,
                "precision": 94.18333333333334,
                "recall": 96.0,
                "main_score": 94.78333333333333
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 83.8785046728972,
                "f1": 80.54517133956385,
                "precision": 79.154984423676,
                "recall": 83.8785046728972,
                "main_score": 80.54517133956385
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 93.60000000000001,
                "f1": 92.01333333333334,
                "precision": 91.28333333333333,
                "recall": 93.60000000000001,
                "main_score": 92.01333333333334
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.1,
                "f1": 96.26666666666667,
                "precision": 95.85000000000001,
                "recall": 97.1,
                "main_score": 96.26666666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 84.3,
                "f1": 80.67833333333333,
                "precision": 79.03928571428571,
                "recall": 84.3,
                "main_score": 80.67833333333333
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.3,
                "f1": 96.48333333333332,
                "precision": 96.08333333333331,
                "recall": 97.3,
                "main_score": 96.48333333333332
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.7,
                "f1": 94.66666666666667,
                "precision": 94.16666666666667,
                "recall": 95.7,
                "main_score": 94.66666666666667
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 97.2,
                "f1": 96.36666666666667,
                "precision": 95.96666666666668,
                "recall": 97.2,
                "main_score": 96.36666666666667
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 94.3,
                "f1": 92.80666666666667,
                "precision": 92.12833333333333,
                "recall": 94.3,
                "main_score": 92.80666666666667
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.0,
                "f1": 96.22333333333334,
                "precision": 95.875,
                "recall": 97.0,
                "main_score": 96.22333333333334
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 74.33333333333333,
                "f1": 70.78174603174602,
                "precision": 69.28333333333332,
                "recall": 74.33333333333333,
                "main_score": 70.78174603174602
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 37.6,
                "f1": 32.938348952090365,
                "precision": 31.2811038961039,
                "recall": 37.6,
                "main_score": 32.938348952090365
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 91.5,
                "f1": 89.13333333333333,
                "precision": 88.03333333333333,
                "recall": 91.5,
                "main_score": 89.13333333333333
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 82.14285714285714,
                "f1": 77.67857142857143,
                "precision": 75.59523809523809,
                "recall": 82.14285714285714,
                "main_score": 77.67857142857143
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 69.0450054884742,
                "f1": 63.070409283362075,
                "precision": 60.58992781824835,
                "recall": 69.0450054884742,
                "main_score": 63.070409283362075
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 63.1,
                "f1": 57.848333333333336,
                "precision": 55.69500000000001,
                "recall": 63.1,
                "main_score": 57.848333333333336
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.1,
                "f1": 95.01666666666667,
                "precision": 94.5,
                "recall": 96.1,
                "main_score": 95.01666666666667
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.89999999999999,
                "f1": 94.90666666666667,
                "precision": 94.425,
                "recall": 95.89999999999999,
                "main_score": 94.90666666666667
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 87.6,
                "f1": 84.61333333333333,
                "precision": 83.27,
                "recall": 87.6,
                "main_score": 84.61333333333333
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 76.4,
                "f1": 71.90746031746032,
                "precision": 70.07027777777778,
                "recall": 76.4,
                "main_score": 71.90746031746032
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.89999999999999,
                "f1": 97.26666666666667,
                "precision": 96.95,
                "recall": 97.89999999999999,
                "main_score": 97.26666666666667
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 78.8,
                "f1": 74.39555555555555,
                "precision": 72.59416666666667,
                "recall": 78.8,
                "main_score": 74.39555555555555
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.19999999999999,
                "f1": 93.78999999999999,
                "precision": 93.125,
                "recall": 95.19999999999999,
                "main_score": 93.78999999999999
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.8,
                "f1": 97.1,
                "precision": 96.75,
                "recall": 97.8,
                "main_score": 97.1
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.6,
                "f1": 94.25666666666666,
                "precision": 93.64166666666668,
                "recall": 95.6,
                "main_score": 94.25666666666666
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 56.934306569343065,
                "f1": 51.461591936044485,
                "precision": 49.37434827945776,
                "recall": 56.934306569343065,
                "main_score": 51.461591936044485
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 20.200000000000003,
                "f1": 16.91799284049284,
                "precision": 15.791855158730158,
                "recall": 20.200000000000003,
                "main_score": 16.91799284049284
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.2,
                "f1": 95.3,
                "precision": 94.85,
                "recall": 96.2,
                "main_score": 95.3
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 96.3,
                "f1": 95.11666666666667,
                "precision": 94.53333333333333,
                "recall": 96.3,
                "main_score": 95.11666666666667
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 89.88095238095238,
                "f1": 87.14285714285714,
                "precision": 85.96230158730161,
                "recall": 89.88095238095238,
                "main_score": 87.14285714285714
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 24.099999999999998,
                "f1": 19.630969083349783,
                "precision": 18.275094905094907,
                "recall": 24.099999999999998,
                "main_score": 19.630969083349783
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 83.4368530020704,
                "f1": 79.45183870649709,
                "precision": 77.7432712215321,
                "recall": 83.4368530020704,
                "main_score": 79.45183870649709
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 95.8,
                "f1": 94.53333333333333,
                "precision": 93.91666666666666,
                "recall": 95.8,
                "main_score": 94.53333333333333
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 98.8,
                "f1": 98.48333333333332,
                "precision": 98.33333333333334,
                "recall": 98.8,
                "main_score": 98.48333333333332
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 17.5,
                "f1": 14.979285714285714,
                "precision": 14.23235060690943,
                "recall": 17.5,
                "main_score": 14.979285714285714
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 93.93939393939394,
                "f1": 91.991341991342,
                "precision": 91.05339105339105,
                "recall": 93.93939393939394,
                "main_score": 91.991341991342
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 89.31297709923665,
                "f1": 86.76844783715012,
                "precision": 85.63613231552164,
                "recall": 89.31297709923665,
                "main_score": 86.76844783715012
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 99.12663755458514,
                "f1": 98.93255701115964,
                "precision": 98.83551673944687,
                "recall": 99.12663755458514,
                "main_score": 98.93255701115964
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.0,
                "f1": 89.77999999999999,
                "precision": 88.78333333333333,
                "recall": 92.0,
                "main_score": 89.77999999999999
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.89265536723164,
                "f1": 95.85687382297553,
                "precision": 95.33898305084746,
                "recall": 96.89265536723164,
                "main_score": 95.85687382297553
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 14.6,
                "f1": 11.820611790170615,
                "precision": 11.022616224355355,
                "recall": 14.6,
                "main_score": 11.820611790170615
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.89999999999999,
                "f1": 94.93333333333334,
                "precision": 94.48666666666666,
                "recall": 95.89999999999999,
                "main_score": 94.93333333333334
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 87.6,
                "f1": 84.72333333333334,
                "precision": 83.44166666666666,
                "recall": 87.6,
                "main_score": 84.72333333333334
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 94.8,
                "f1": 93.47333333333333,
                "precision": 92.875,
                "recall": 94.8,
                "main_score": 93.47333333333333
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.6,
                "f1": 95.71666666666665,
                "precision": 95.28333333333335,
                "recall": 96.6,
                "main_score": 95.71666666666665
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 17.8,
                "f1": 14.511074040901628,
                "precision": 13.503791000666002,
                "recall": 17.8,
                "main_score": 14.511074040901628
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 94.10187667560321,
                "f1": 92.46648793565683,
                "precision": 91.71134941912423,
                "recall": 94.10187667560321,
                "main_score": 92.46648793565683
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 97.0,
                "f1": 96.11666666666666,
                "precision": 95.68333333333334,
                "recall": 97.0,
                "main_score": 96.11666666666666
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 72.72727272727273,
                "f1": 66.58949745906267,
                "precision": 63.86693017127799,
                "recall": 72.72727272727273,
                "main_score": 66.58949745906267
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 90.14084507042254,
                "f1": 88.26291079812206,
                "precision": 87.32394366197182,
                "recall": 90.14084507042254,
                "main_score": 88.26291079812206
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 64.67065868263472,
                "f1": 58.2876627696987,
                "precision": 55.79255774165953,
                "recall": 64.67065868263472,
                "main_score": 58.2876627696987
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 95.6,
                "f1": 94.41666666666667,
                "precision": 93.85,
                "recall": 95.6,
                "main_score": 94.41666666666667
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 55.172413793103445,
                "f1": 49.63992493549144,
                "precision": 47.71405113769646,
                "recall": 55.172413793103445,
                "main_score": 49.63992493549144
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 77.46478873239437,
                "f1": 73.4417616811983,
                "precision": 71.91607981220658,
                "recall": 77.46478873239437,
                "main_score": 73.4417616811983
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 84.61538461538461,
                "f1": 80.91452991452994,
                "precision": 79.33760683760683,
                "recall": 84.61538461538461,
                "main_score": 80.91452991452994
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 98.2,
                "f1": 97.6,
                "precision": 97.3,
                "recall": 98.2,
                "main_score": 97.6
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 75.5741127348643,
                "f1": 72.00417536534445,
                "precision": 70.53467872883321,
                "recall": 75.5741127348643,
                "main_score": 72.00417536534445
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 62.2,
                "f1": 55.577460317460314,
                "precision": 52.98583333333333,
                "recall": 62.2,
                "main_score": 55.577460317460314
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 92.18241042345277,
                "f1": 90.6468124709167,
                "precision": 89.95656894679696,
                "recall": 92.18241042345277,
                "main_score": 90.6468124709167
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.1,
                "f1": 95.13333333333333,
                "precision": 94.66666666666667,
                "recall": 96.1,
                "main_score": 95.13333333333333
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 96.8,
                "f1": 95.85000000000001,
                "precision": 95.39999999999999,
                "recall": 96.8,
                "main_score": 95.85000000000001
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 92.1259842519685,
                "f1": 89.76377952755905,
                "precision": 88.71391076115485,
                "recall": 92.1259842519685,
                "main_score": 89.76377952755905
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 94.1,
                "f1": 92.49,
                "precision": 91.725,
                "recall": 94.1,
                "main_score": 92.49
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 77.5623268698061,
                "f1": 73.27364463791058,
                "precision": 71.51947852086357,
                "recall": 77.5623268698061,
                "main_score": 73.27364463791058
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.39999999999999,
                "f1": 96.56666666666666,
                "precision": 96.16666666666667,
                "recall": 97.39999999999999,
                "main_score": 96.56666666666666
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 66.34615384615384,
                "f1": 61.092032967032964,
                "precision": 59.27197802197802,
                "recall": 66.34615384615384,
                "main_score": 61.092032967032964
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 94.89999999999999,
                "f1": 93.41190476190476,
                "precision": 92.7,
                "recall": 94.89999999999999,
                "main_score": 93.41190476190476
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 93.10000000000001,
                "f1": 91.10000000000001,
                "precision": 90.13333333333333,
                "recall": 93.10000000000001,
                "main_score": 91.10000000000001
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 93.7,
                "f1": 91.97333333333334,
                "precision": 91.14166666666667,
                "recall": 93.7,
                "main_score": 91.97333333333334
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 92.21698113207547,
                "f1": 90.3796046720575,
                "precision": 89.56367924528303,
                "recall": 92.21698113207547,
                "main_score": 90.3796046720575
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 97.6,
                "f1": 96.91666666666667,
                "precision": 96.6,
                "recall": 97.6,
                "main_score": 96.91666666666667
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 97.44525547445255,
                "f1": 96.71532846715328,
                "precision": 96.35036496350365,
                "recall": 97.44525547445255,
                "main_score": 96.71532846715328
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 94.1,
                "f1": 92.34000000000002,
                "precision": 91.49166666666667,
                "recall": 94.1,
                "main_score": 92.34000000000002
            }
        ]
    }
}