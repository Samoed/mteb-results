{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": NaN,
    "mteb_version": "unknown",
    "scores": {
        "test": [
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 84.49562878278414,
                "f1": 84.0040193313333,
                "main_score": 84.49562878278414
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 80.76328177538669,
                "f1": 80.24718532423358,
                "main_score": 80.76328177538669
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 76.37188971082716,
                "f1": 75.64847309941361,
                "main_score": 76.37188971082716
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 79.09885675857431,
                "f1": 78.28407777434224,
                "main_score": 79.09885675857431
            }
        ]
    }
}