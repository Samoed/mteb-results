{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": -1,
    "mteb_version": "0.0.0",
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 43.2481506388702,
                "f1": 40.924230769590785,
                "main_score": 43.2481506388702
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 25.30262273032952,
                "f1": 24.937105830264066,
                "main_score": 25.30262273032952
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 32.07128446536651,
                "f1": 31.80245816594883,
                "main_score": 32.07128446536651
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 36.681237390719566,
                "f1": 36.37219042508338,
                "main_score": 36.681237390719566
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 29.56624075319435,
                "f1": 28.386042056362758,
                "main_score": 29.56624075319435
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 42.1049092131809,
                "f1": 38.926150886991294,
                "main_score": 42.1049092131809
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 45.44384667114997,
                "f1": 42.578252395460005,
                "main_score": 45.44384667114997
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 43.211163416274374,
                "f1": 41.04465858304789,
                "main_score": 43.211163416274374
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 36.503026227303295,
                "f1": 34.49785095312759,
                "main_score": 36.503026227303295
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 69.73772696704773,
                "f1": 69.21759502909043,
                "main_score": 69.73772696704773
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 44.078681909885674,
                "f1": 43.05914426901129,
                "main_score": 44.078681909885674
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 32.61264290517821,
                "f1": 32.02463177462754,
                "main_score": 32.61264290517821
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 40.35642232683255,
                "f1": 38.13642481807678,
                "main_score": 40.35642232683255
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 45.06724949562878,
                "f1": 43.19827608343738,
                "main_score": 45.06724949562878
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 32.178883658372555,
                "f1": 29.979761884698775,
                "main_score": 32.178883658372555
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 26.903160726294555,
                "f1": 25.833010434083363,
                "main_score": 26.903160726294555
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 40.379959650302624,
                "f1": 37.93134355292882,
                "main_score": 40.379959650302624
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 28.375924680564896,
                "f1": 26.96255693013172,
                "main_score": 28.375924680564896
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 44.361129791526565,
                "f1": 43.54445012295126,
                "main_score": 44.361129791526565
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 39.290517821116346,
                "f1": 37.26982052174147,
                "main_score": 39.290517821116346
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 46.4694014794889,
                "f1": 44.060986162841566,
                "main_score": 46.4694014794889
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 46.25756556825824,
                "f1": 45.625139456758816,
                "main_score": 46.25756556825824
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 41.12642905178212,
                "f1": 39.54392378396527,
                "main_score": 41.12642905178212
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 24.72763954270343,
                "f1": 23.337743140804484,
                "main_score": 24.72763954270343
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 29.741089441829182,
                "f1": 27.570876190083748,
                "main_score": 29.741089441829182
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 23.850033624747816,
                "f1": 22.86733484540032,
                "main_score": 23.850033624747816
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 36.56691324815064,
                "f1": 35.504081677134565,
                "main_score": 36.56691324815064
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 40.928043039677206,
                "f1": 39.108589131211254,
                "main_score": 40.928043039677206
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 25.527908540685946,
                "f1": 25.333391622280477,
                "main_score": 25.527908540685946
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 29.105581708137183,
                "f1": 28.478235012692814,
                "main_score": 29.105581708137183
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 43.78614660390047,
                "f1": 41.9640143926267,
                "main_score": 43.78614660390047
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 27.269670477471415,
                "f1": 26.228386764141852,
                "main_score": 27.269670477471415
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 39.018157363819775,
                "f1": 37.641949339321854,
                "main_score": 39.018157363819775
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 45.35978480161399,
                "f1": 42.6851176096831,
                "main_score": 45.35978480161399
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 41.89307330195023,
                "f1": 40.888710642615024,
                "main_score": 41.89307330195023
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 45.901143241425686,
                "f1": 44.496942353920545,
                "main_score": 45.901143241425686
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 44.11566913248151,
                "f1": 41.953945105870616,
                "main_score": 44.11566913248151
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 32.76395427034297,
                "f1": 31.436372571600934,
                "main_score": 32.76395427034297
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 40.504371217215876,
                "f1": 39.322752749628165,
                "main_score": 40.504371217215876
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 42.51849361129792,
                "f1": 41.4139297118463,
                "main_score": 42.51849361129792
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 42.293207800941495,
                "f1": 40.50409536806683,
                "main_score": 42.293207800941495
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 42.9993275050437,
                "f1": 41.045416224973266,
                "main_score": 42.9993275050437
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 28.32548755884331,
                "f1": 27.276841995561867,
                "main_score": 28.32548755884331
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 26.593813046402154,
                "f1": 25.483878616197586,
                "main_score": 26.593813046402154
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 36.788836583725626,
                "f1": 34.603932909177686,
                "main_score": 36.788836583725626
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 42.5689307330195,
                "f1": 40.924469309079825,
                "main_score": 42.5689307330195
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 37.09482178883658,
                "f1": 37.949628822857164,
                "main_score": 37.09482178883658
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 28.836583725622063,
                "f1": 27.806558655512344,
                "main_score": 28.836583725622063
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 37.357094821788834,
                "f1": 37.507918961038165,
                "main_score": 37.357094821788834
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 49.37794216543375,
                "f1": 47.20421153697707,
                "main_score": 49.37794216543375
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 44.42165433759248,
                "f1": 44.34741861198931,
                "main_score": 44.42165433759248
            }
        ]
    }
}